{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8BHbdFkmHN"
      },
      "source": [
        "# Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPQ4nDrBkklE",
        "outputId": "74c59fab-b281-4094-d76e-115e67b618fb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_vdvA5hdGmP"
      },
      "source": [
        "# Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-9WiyGWdGmP",
        "outputId": "46d7343e-87b5-4217-ad87-b40e8ba6f08c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
        "from huggingface_hub import login\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "from huggingface_hub import hf_hub_download\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ghYgpNxdGmP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5STQ_uCdGmP"
      },
      "source": [
        "# Model Setup and Loading Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LZOrzUCTo4b"
      },
      "outputs": [],
      "source": [
        "class CNNHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNHead, self).__init__()\n",
        "\n",
        "        # Define the transposed convolution layers (upsampling)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Final convolution to adjust to the target size\n",
        "        self.final_conv = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        CNNHead_features = []\n",
        "\n",
        "        # Apply the transposed convolutions (upsampling)\n",
        "        x = torch.relu(self.deconv1(x))\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "        x = torch.relu(self.deconv2(x))\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "        x = torch.relu(self.deconv3(x))\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "        x = self.deconv4(x)\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "        # Final convolution to adjust to the target size\n",
        "        x = self.final_conv(x)\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "        # Add interpolation layer to achieve the target size\n",
        "        x = F.interpolate(x, size=(600, 600), mode='bilinear', align_corners=False)  # [B, 1, 1151, 1151] -> [B, 1, 600, 600]\n",
        "        CNNHead_features.append(x)\n",
        "\n",
        "\n",
        "        return CNNHead_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN-Ubd8QTqo5"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels=[64, 128, 256, 256]):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        # Define the convolutional layers (feature extraction layers)\n",
        "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels=conv_channels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv_channels[0], out_channels=conv_channels[1], kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=conv_channels[1], out_channels=conv_channels[2], kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=conv_channels[2], out_channels=conv_channels[3], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ConvBlock_features = []\n",
        "\n",
        "        # Apply the convolutional blocks sequentially\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv4(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "\n",
        "        # Return the feature map after the last convolutional layer\n",
        "        return ConvBlock_features\n",
        "        self.conv4 = nn.Conv2d(conv_channels[2], conv_channels[3], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ConvBlock_features = []\n",
        "\n",
        "        # Apply the convolutional blocks sequentially\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv4(x)))\n",
        "        ConvBlock_features.append(x)\n",
        "\n",
        "\n",
        "        # Return the feature map after the last convolutional layer\n",
        "        return ConvBlock_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlCtVt2VTsDi"
      },
      "outputs": [],
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=32):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "\n",
        "        # Conv1: [B, 3, 600, 600] -> [B, 16, 300, 300]\n",
        "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Conv2: [B, 16, 300, 300] -> [B, 32, 600, 600]\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        CNNEncoder_features = []\n",
        "\n",
        "        # Pass through conv1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        CNNEncoder_features.append(x)\n",
        "\n",
        "        # Pass through conv2 (with upsampling)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        CNNEncoder_features.append(x)\n",
        "\n",
        "        return CNNEncoder_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U8SdKL8aNLO"
      },
      "outputs": [],
      "source": [
        "class AOHead(nn.Module):\n",
        "    def __init__(self, in_channels=768, out_channels=1):\n",
        "        super(AOHead, self).__init__()\n",
        "\n",
        "        # Gradual upsampling with ConvTranspose2d layers\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels, 512, kernel_size=3, stride=2, padding=1, output_padding=1)  # [B, 768, 18, 18] -> [B, 512, 36, 36]\n",
        "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)         # [B, 512, 36, 36] -> [B, 256, 72, 72]\n",
        "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)         # [B, 256, 72, 72] -> [B, 128, 144, 144]\n",
        "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)          # [B, 128, 144, 144] -> [B, 64, 288, 288]\n",
        "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)           # [B, 64, 288, 288] -> [B, 32, 576, 576]\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()  # Normalize to range [0, 1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the AO head.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): The input feature map from the backbone.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The predicted Ambient Occlusion map.\n",
        "        \"\"\"\n",
        "        head_features = []\n",
        "\n",
        "        # Gradual upsampling with ConvTranspose2d layers\n",
        "        x = self.relu(self.deconv1(x))    # [B, 768, 18, 18] -> [B, 512, 36, 36]\n",
        "        head_features.append(x)\n",
        "\n",
        "        x = self.relu(self.deconv2(x))    # [B, 512, 36, 36] -> [B, 256, 72, 72]\n",
        "        head_features.append(x)\n",
        "\n",
        "        x = self.relu(self.deconv3(x))    # [B, 256, 72, 72] -> [B, 128, 144, 144]\n",
        "        head_features.append(x)\n",
        "\n",
        "        x = self.relu(self.deconv4(x))    # [B, 128, 144, 144] -> [B, 64, 288, 288]\n",
        "        head_features.append(x)\n",
        "\n",
        "        x = self.relu(self.deconv5(x))    # [B, 64, 288, 288] -> [B, 32, 576, 576]\n",
        "        head_features.append(x)\n",
        "\n",
        "        # Add interpolation layer to achieve the target size\n",
        "        x = F.interpolate(x, size=(600, 600), mode='bilinear', align_corners=False)  # [B, 1, 1151, 1151] -> [B, 1, 600, 600]\n",
        "        head_features.append(x)\n",
        "\n",
        "\n",
        "        return head_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMbPN18j7zFM"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lcj44ZF709R"
      },
      "outputs": [],
      "source": [
        "class ConvNeXt(nn.Module):\n",
        "    r\"\"\" ConvNeXt\n",
        "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
        "          https://arxiv.org/pdf/2201.03545.pdf\n",
        "    Args:\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans=3, out_chans=1,\n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 **kwargs,):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "\n",
        "\n",
        "        # Output of AOHead part\n",
        "        self.ao_head = AOHead(in_channels= dims[-1], out_channels=1)\n",
        "\n",
        "        # Output of CNNEncoder part\n",
        "        self.cnn_encoder = CNNEncoder(in_channels= 3, out_channels=32)\n",
        "\n",
        "        # Output of ConvBlock part\n",
        "        self.conv_block = ConvBlock(in_channels= 64, conv_channels=[64, 128, 256, 256])\n",
        "\n",
        "        # Output of CNNHead part\n",
        "        self.cnn_head = CNNHead()\n",
        "\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        features = []\n",
        "        input = x\n",
        "\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "            features.append(x)\n",
        "\n",
        "        # Output of AOHead part\n",
        "        head_features = self.ao_head(x)\n",
        "        features.extend(head_features)\n",
        "\n",
        "        x = head_features[-1]\n",
        "\n",
        "\n",
        "        # Output of CNNEncoder part\n",
        "        cnn_encoder_features = self.cnn_encoder(input)\n",
        "        features.extend(cnn_encoder_features)\n",
        "\n",
        "        x = cnn_encoder_features[-1]\n",
        "\n",
        "\n",
        "        # Output of ConvBlock part\n",
        "\n",
        "        # Concatenate along the channel dimension (dim=1)\n",
        "        concatenated_tensor = torch.cat((head_features[-1], cnn_encoder_features[-1]), dim=1)\n",
        "\n",
        "        conv_block_features = self.conv_block(concatenated_tensor)\n",
        "        features.extend(conv_block_features)\n",
        "\n",
        "        x = conv_block_features[-1]\n",
        "\n",
        "        # Output of CNNHead part\n",
        "        CNNHead_features = self.cnn_head(x)\n",
        "        features.extend(CNNHead_features)\n",
        "\n",
        "        x = CNNHead_features[-1]\n",
        "\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.forward_features(x)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuYfOnr9wTZG"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnwIBS-j74JO"
      },
      "outputs": [],
      "source": [
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
        "    \"AO_Model\": \"https://huggingface.co/prakanda/AO_Model/resolve/main/AO_Model.pth\",\n",
        "    \"New_AO_Model_1\": \"https://huggingface.co/prakanda/New_AO_Model_1/resolve/main/New_AO_Model_1.pth\",\n",
        " #   \"ConvNeXt_ambient_occlusion_model_1\" : \"https://huggingface.co/prakanda/ConvNeXt_ambient_occlusion_model_1/resolve/main/ConvNeXt_ambient_occlusion_model_1.pth\",\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8AZOSbU76s9"
      },
      "outputs": [],
      "source": [
        "def convnext_tiny(pretrained=True,load_ao=True,**kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        #checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
        "\n",
        "        if load_ao:\n",
        "            url = model_urls['New_AO_Model_1']\n",
        "            print(\"AO_Model has been loaded.\" )\n",
        "        else:\n",
        "            url = model_urls['convnext_tiny_1k']\n",
        "            print(\"convnext_tiny_1k has been loaded\")\n",
        "\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
        "\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {}\n",
        "        unmatched_pretrained_dict = {}\n",
        "\n",
        "        for k, v in checkpoint.items():\n",
        "            if k in model_dict:\n",
        "                pretrained_dict[k] = v\n",
        "            else:\n",
        "                unmatched_pretrained_dict[k] = v\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "\n",
        "\n",
        "        print(f'The type of checkpoint is {type(checkpoint)}.')\n",
        "        print(f'The type of model.state_dict is {type(model.state_dict)}.')\n",
        "        print(f'The type of model.state_dict() is {type(model.state_dict())}.')\n",
        "\n",
        "\n",
        "\n",
        "        for name,param in model.named_parameters():\n",
        "          if name in pretrained_dict.keys():\n",
        "              param.requires_grad = True\n",
        "          else :\n",
        "              param.requires_grad = True\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        print(f'The keys in pretrained_dict are : \\n {pretrained_dict.keys()}')\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        print(f'The keys in unmatched_pretrained_dict are : \\n {unmatched_pretrained_dict.keys()}')\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        print(\n",
        "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
        "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nybf7QtwdGmQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO6N23bzelMN"
      },
      "source": [
        "# Model Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y031UgzuejQR",
        "outputId": "eac958bf-ed9f-4788-b841-18bcf1aaf991"
      },
      "outputs": [],
      "source": [
        "model = convnext_tiny(True, load_ao=True).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61sySczYejsG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBtKqrCidGmW"
      },
      "source": [
        "# Model Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "45498f7012d1449c94f8c8c91e546598",
            "d57c6f290a434262a0c44f864ada4519",
            "82115dca222547678c8655889f3c9b64",
            "798194e9e52945e7afbad4801094b24e",
            "44de6fd5c9174f37880ca27a0d750b51",
            "708f9874e52e429696c8de1c1de9a3dd",
            "d0ef73416e2e452c96b5e46ac9cb26b2",
            "3cf7d165923f4841a71be532d53e7577",
            "1a9786bb88654c9dbd75933d46e3c44c",
            "d21a974399664397842821208e96419e",
            "ba43bbf5a99549209657a15fe806d4bf"
          ]
        },
        "id": "-JTXXA0s1D4-",
        "outputId": "728b833e-537d-48ac-8840-b4112b97e6fc"
      },
      "outputs": [],
      "source": [
        "# Download the model file from Hugging Face Hub\n",
        "repo_name = \"New_AO_Model_1\"\n",
        "downloaded_file = hf_hub_download(\n",
        "    repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
        "    filename=\"New_AO_Model_1.pth\"\n",
        ")\n",
        "print(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n",
        "\n",
        "\n",
        "# Initialize the model and load the state_dict\n",
        "model.load_state_dict(torch.load(downloaded_file),strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e1uQs3kD2yW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cHqWq8UD4dJ"
      },
      "source": [
        "# Result Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7bA4FnF9mN-"
      },
      "outputs": [],
      "source": [
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((600, 600)),\n",
        "        transforms.ToTensor(),          # Convert the image to a PyTorch tensor (HWC -> CHW, normalized [0, 1])\n",
        "    ])\n",
        "\n",
        "\n",
        "def path_to_tensor(image):\n",
        "  image = Image.open(image).convert(\"RGB\")\n",
        "  tensor_image = transform(image)\n",
        "  tensor_image = tensor_image.unsqueeze(0)\n",
        "  return tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7UpOOBr9qVh"
      },
      "outputs": [],
      "source": [
        "def inference(path):\n",
        "    input_tensor = path_to_tensor(path)\n",
        "    input_tensor = input_tensor.to(\"cuda\")\n",
        "\n",
        "    # Example size, adjust as needed\n",
        "    dic={\"input\":input_tensor}\n",
        "\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation for evaluation/testing\n",
        "        output = model(dic[\"input\"])\n",
        "        return output[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2A6U9V8w9w3P",
        "outputId": "e235ad65-6eb3-4f84-8ff1-21244ae8d9f1"
      },
      "outputs": [],
      "source": [
        "filenames=glob(\"/content/drive/MyDrive/AO/*\")\n",
        "\n",
        "\n",
        "# Assuming filenames is a list of file paths\n",
        "for filepath in filenames:\n",
        "    output = inference(filepath)\n",
        "    image = output.to(\"cpu\")\n",
        "\n",
        "    # Squeeze batch and channel dimensions, then convert to numpy array\n",
        "    ambient_image = image.squeeze(0).squeeze(0).numpy()\n",
        "\n",
        "    # Load the original image from the file path\n",
        "    original_image_path = Image.open(filepath)\n",
        "\n",
        "    # Create a subplot with 2 images\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plot the original image (from the file)\n",
        "    axs[0].imshow(original_image_path, cmap='gray')\n",
        "    axs[0].set_title(\"Original Image from Path\")\n",
        "    axs[0].axis('off')  # Hide axes for better visualization\n",
        "\n",
        "    # Plot the processed image (inverse normalized)\n",
        "    axs[1].imshow(ambient_image, cmap='gray')\n",
        "    axs[1].set_title(\"Ambient Occlusion Map\")\n",
        "    axs[1].axis('off')  # Hide axes for better visualization\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6211511,
          "sourceId": 10076639,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6220263,
          "sourceId": 10088214,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6220624,
          "sourceId": 10088660,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6222017,
          "sourceId": 10090456,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6256336,
          "sourceId": 10137083,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a9786bb88654c9dbd75933d46e3c44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cf7d165923f4841a71be532d53e7577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44de6fd5c9174f37880ca27a0d750b51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45498f7012d1449c94f8c8c91e546598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d57c6f290a434262a0c44f864ada4519",
              "IPY_MODEL_82115dca222547678c8655889f3c9b64",
              "IPY_MODEL_798194e9e52945e7afbad4801094b24e"
            ],
            "layout": "IPY_MODEL_44de6fd5c9174f37880ca27a0d750b51"
          }
        },
        "708f9874e52e429696c8de1c1de9a3dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798194e9e52945e7afbad4801094b24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21a974399664397842821208e96419e",
            "placeholder": "​",
            "style": "IPY_MODEL_ba43bbf5a99549209657a15fe806d4bf",
            "value": " 139M/139M [00:03&lt;00:00, 42.6MB/s]"
          }
        },
        "82115dca222547678c8655889f3c9b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf7d165923f4841a71be532d53e7577",
            "max": 138539162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a9786bb88654c9dbd75933d46e3c44c",
            "value": 138539162
          }
        },
        "ba43bbf5a99549209657a15fe806d4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0ef73416e2e452c96b5e46ac9cb26b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d21a974399664397842821208e96419e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57c6f290a434262a0c44f864ada4519": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_708f9874e52e429696c8de1c1de9a3dd",
            "placeholder": "​",
            "style": "IPY_MODEL_d0ef73416e2e452c96b5e46ac9cb26b2",
            "value": "New_AO_Model_1.pth: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
