{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-15T07:56:12.272901Z",
     "iopub.status.busy": "2024-12-15T07:56:12.272649Z",
     "iopub.status.idle": "2024-12-15T07:56:22.010124Z",
     "shell.execute_reply": "2024-12-15T07:56:22.009278Z",
     "shell.execute_reply.started": "2024-12-15T07:56:12.272876Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning and Switching to the `additional_training` Branch in Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:32.164744Z",
     "iopub.status.busy": "2024-12-15T07:57:32.164195Z",
     "iopub.status.idle": "2024-12-15T07:57:32.17077Z",
     "shell.execute_reply": "2024-12-15T07:57:32.169741Z",
     "shell.execute_reply.started": "2024-12-15T07:57:32.164708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:32.383264Z",
     "iopub.status.busy": "2024-12-15T07:57:32.382936Z",
     "iopub.status.idle": "2024-12-15T07:57:34.236446Z",
     "shell.execute_reply": "2024-12-15T07:57:34.235284Z",
     "shell.execute_reply.started": "2024-12-15T07:57:32.383236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/prathamadh/Texture_training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:34.238665Z",
     "iopub.status.busy": "2024-12-15T07:57:34.23837Z",
     "iopub.status.idle": "2024-12-15T07:57:34.244641Z",
     "shell.execute_reply": "2024-12-15T07:57:34.24379Z",
     "shell.execute_reply.started": "2024-12-15T07:57:34.238638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/Texture_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:34.245944Z",
     "iopub.status.busy": "2024-12-15T07:57:34.24568Z",
     "iopub.status.idle": "2024-12-15T07:57:36.58614Z",
     "shell.execute_reply": "2024-12-15T07:57:36.585224Z",
     "shell.execute_reply.started": "2024-12-15T07:57:34.24592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git fetch origin\n",
    "!git checkout additional_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:36.58841Z",
     "iopub.status.busy": "2024-12-15T07:57:36.588102Z",
     "iopub.status.idle": "2024-12-15T07:57:37.592475Z",
     "shell.execute_reply": "2024-12-15T07:57:37.591583Z",
     "shell.execute_reply.started": "2024-12-15T07:57:36.588383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:37.594117Z",
     "iopub.status.busy": "2024-12-15T07:57:37.59379Z",
     "iopub.status.idle": "2024-12-15T07:57:37.600633Z",
     "shell.execute_reply": "2024-12-15T07:57:37.599714Z",
     "shell.execute_reply.started": "2024-12-15T07:57:37.594089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
    "from huggingface_hub import login\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup and Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n",
    "    super(DoubleConv, self).__init__()\n",
    "    self.conv_op = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.conv_op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, concat_channels, out_channels, kernel_size, stride ,padding, output_padding = 0):\n",
    "        super(UpSample, self).__init__()\n",
    "\n",
    "        # Deconvolutional layers\n",
    "        self.up = nn.ConvTranspose2d(in_channels, concat_channels, kernel_size, stride, padding, output_padding)\n",
    "        self.conv = DoubleConv(concat_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.conv(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, out_chans=1,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 **kwargs,):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "\n",
    "\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "\n",
    "\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "\n",
    "        cur = 0\n",
    "\n",
    "\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "\n",
    "\n",
    "            self.stages.append(stage)\n",
    "\n",
    "            cur += depths[i]\n",
    "\n",
    "\n",
    "\n",
    "        # Output head for roughness map prediction\n",
    "        # Upconvolutions\n",
    "        self.upconvolution_1 = UpSample(dims[-1], dims[-1] // 2, dims[-1] // 4, kernel_size= 4, stride=2, padding = 1, output_padding = 1)\n",
    "        self.upconvolution_2 = UpSample(dims[-2], dims[-2] // 2, dims[-2] // 4, kernel_size= 2, stride=2, padding = 0)\n",
    "        self.upconvolution_3 = UpSample(dims[-3], dims[-3] // 2, dims[-3] // 4, kernel_size= 2, stride=2, padding = 0)\n",
    "        self.upconvolution_4 = UpSample(dims[-4], dims[-4] // 2, dims[-4] // 4, kernel_size= 2, stride=2, padding = 0)\n",
    "\n",
    "\n",
    "\n",
    "        # Pixel Shuffles\n",
    "        self.pixel_shuffle_1 = nn.PixelShuffle(2)\n",
    "        self.pixel_shuffle_2 = nn.PixelShuffle(2)\n",
    "        self.pixel_shuffle_3 = nn.PixelShuffle(2)\n",
    "        self.pixel_shuffle_4 = nn.PixelShuffle(2)\n",
    "\n",
    "\n",
    "        # Intermediate Upconcolutions\n",
    "        self.inter_upconv_1_1 = UpSample(dims[-2], dims[-3] , dims[-3], kernel_size=2, stride = 2, padding = 0)\n",
    "        self.inter_upconv_1_2 = UpSample(dims[-2], dims[-4] , dims[-4], kernel_size=2, stride = 2, padding = 0)\n",
    "        self.inter_upconv_2_3 = UpSample(dims[-3], dims[-4] , dims[-4], kernel_size=2, stride = 2, padding = 0)\n",
    "        self.inter_upconv_3_4 = UpSample(dims[-3] , dims[-4], dims[-4], kernel_size=2, stride = 2, padding = 0)\n",
    "\n",
    "\n",
    "        # Final deconvolution layers\n",
    "        self.f_pixel_shuffle = nn.PixelShuffle(2)\n",
    "\n",
    "        # Final Convolutions\n",
    "        self.f_douconv1 = DoubleConv(64, 32)\n",
    "        self.f_douconv2 = DoubleConv(32, 16)\n",
    "        self.f_douconv3 = DoubleConv(16, 8)\n",
    "        self.f_douconv4 = DoubleConv(8, 4)\n",
    "        self.f_douconv5 = DoubleConv(4, 2)\n",
    "        self.f_douconv6 = DoubleConv(2, 1)\n",
    "\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        features = []\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "            features.append(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Obtaining output from each steps\n",
    "        stage_1_op = features[0]\n",
    "        stage_2_op = features[1]\n",
    "        stage_3_op = features[2]\n",
    "        stage_4_op = features[3]\n",
    "\n",
    "\n",
    "        #print('Stage 1 Output Shape:', stage_1_op.shape)\n",
    "        #print('Stage 2 Output Shape:', stage_2_op.shape)\n",
    "        #print('Stage 3 Output Shape:', stage_3_op.shape)\n",
    "        #print('Stage 4 Output Shape:', stage_4_op.shape)\n",
    "\n",
    "\n",
    "\n",
    "        # Upsampling steps\n",
    "\n",
    "        # For outputs from stage 4\n",
    "        a1 = self.pixel_shuffle_1(stage_4_op)\n",
    "        features.append(a1)\n",
    "\n",
    "        #print('Stage a1 Output Shape:', features[-1].shape)\n",
    "\n",
    "        b1 = self.upconvolution_1(stage_4_op)\n",
    "        features.append(b1)\n",
    "        #print('Stage b1 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        # Upsample to 37x37 using bilinear interpolation\n",
    "        a1 = F.interpolate(a1, size=(37, 37), mode='bilinear', align_corners=False)\n",
    "        features.append(a1)\n",
    "        #print('Stage a1 Output Shape after interpolation:', features[-1].shape)\n",
    "\n",
    "        ab1 = torch.cat((a1,b1), dim = 1)\n",
    "        features.append(ab1)\n",
    "        #print('Stage ab1 Output Shape:', features[-1].shape)\n",
    "\n",
    "        ab1 = self.inter_upconv_1_1(ab1)        \n",
    "        features.append(ab1)\n",
    "        #print('Stage ab1 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "\n",
    "        # For outputs from stage 3\n",
    "        a2 = self.pixel_shuffle_2(stage_3_op)\n",
    "        features.append(a2)\n",
    "        #print('Stage a2 Output Shape:', features[-1].shape)\n",
    "\n",
    "        b2 = self.upconvolution_2(stage_3_op)\n",
    "        features.append(b2)\n",
    "        #print('Stage b2 Output Shape:', features[-1].shape)\n",
    "\n",
    "        ab2 = torch.cat((a2,b2), dim = 1)\n",
    "        features.append(ab2)\n",
    "        #print('Stage ab2 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        ab12 = torch.cat((ab1, ab2), dim = 1)\n",
    "        features.append(ab12)\n",
    "        #print('Stage ab12 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        ab12 = self.inter_upconv_1_2(ab12)\n",
    "        features.append(ab12)\n",
    "        #print('Stage ab12 Output Shape:', features[-1].shape)\n",
    "\n",
    "        # Upsample to 37x37 using bilinear interpolation\n",
    "        ab12 = F.interpolate(ab12, size=(150, 150), mode='bilinear', align_corners=False)\n",
    "        features.append(ab12)\n",
    "        #print('Stage ab12 Output Shape after interpolation:', features[-1].shape)\n",
    "\n",
    "\n",
    "\n",
    "        # For outputs from stage 2\n",
    "        a3 = self.pixel_shuffle_3(stage_2_op)\n",
    "        features.append(a3)\n",
    "        #print('Stage a3 Output Shape:', features[-1].shape)\n",
    "        \n",
    "        b3 = self.upconvolution_3(stage_2_op)\n",
    "        features.append(b3)\n",
    "        #print('Stage b3 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        ab3 = torch.cat((a3,b3), dim = 1)\n",
    "        features.append(ab3)\n",
    "        #print('Stage ab3 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "\n",
    "        ab3_12 = torch.cat((ab3, ab12), dim = 1)\n",
    "        features.append(ab3_12)\n",
    "        #print('Stage ab3_12 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        ab3_12 = self.inter_upconv_2_3(ab3_12)\n",
    "        features.append(ab3_12)\n",
    "        #print('Stage ab3_12 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # For outputs from stage 1\n",
    "        a4 = self.pixel_shuffle_4(stage_1_op)\n",
    "        features.append(a4)\n",
    "        #print('Stage a4 Output Shape:', features[-1].shape)\n",
    "\n",
    "        b4 = self.upconvolution_4(stage_1_op)\n",
    "        features.append(b4)\n",
    "        #print('Stage b4 Output Shape:', features[-1].shape)\n",
    "\n",
    "        ab4 = torch.cat((a4,b4), dim = 1)\n",
    "        features.append(ab4)\n",
    "        #print('Stage ab4 Output Shape:', features[-1].shape)\n",
    "\n",
    "        ab4_3_12 = torch.cat((ab4, ab3_12), dim = 1)\n",
    "        features.append(ab4_3_12)\n",
    "        #print('Stage ab4_3_12 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        ab4_3_12 = torch.cat((ab4, ab4_3_12), dim = 1)\n",
    "        features.append(ab4_3_12)\n",
    "        #print('Stage ab4_3_12 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        # Final deconvolution layer\n",
    "        f_ps_1 = self.f_pixel_shuffle(ab4_3_12)\n",
    "        features.append(f_ps_1)\n",
    "        #print('Stage f_ps_1 Output Shape:', features[-1].shape)\n",
    "\n",
    "        f_doucov_1 = self.f_douconv1(f_ps_1)\n",
    "        features.append(f_doucov_1)\n",
    "        #print('Stage f_doucov_1 Output Shape:', features[-1].shape)\n",
    "\n",
    "        f_doucov_2 = self.f_douconv2(f_doucov_1)\n",
    "        features.append(f_doucov_2)\n",
    "        #print('Stage f_doucov_2 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        f_doucov_3 = self.f_douconv3(f_doucov_2)\n",
    "        features.append(f_doucov_3)\n",
    "        #print('Stage f_doucov_3 Output Shape:', features[-1].shape)\n",
    "\n",
    "        f_doucov_4 = self.f_douconv4(f_doucov_3)\n",
    "        features.append(f_doucov_4)\n",
    "        #print('Stage f_doucov_4 Output Shape:', features[-1].shape)\n",
    "\n",
    "        f_doucov_5 = self.f_douconv5(f_doucov_4)\n",
    "        features.append(f_doucov_5)\n",
    "        #print('Stage f_doucov_5 Output Shape:', features[-1].shape)\n",
    "\n",
    "        f_doucov_6 = self.f_douconv6(f_doucov_5)\n",
    "        features.append(f_doucov_6)\n",
    "        #print('Stage f_doucov_6 Output Shape:', features[-1].shape)\n",
    "\n",
    "\n",
    "        x = f_doucov_6\n",
    "        \n",
    "        features.append(x)\n",
    "        #print('Final Output Shape:', features[-1].shape)\n",
    "\n",
    "        return features\n",
    "        # return features # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.forward_features(x)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convnext_base(pretrained=True, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
    "    if pretrained:\n",
    "        # checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # print(model_dict)\n",
    "        # print(checkpoint)\n",
    "\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict.keys():\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        print(f'The type of checkpoint is {type(checkpoint)}.')\n",
    "        print(f'The type of model.state_dict is {type(model.state_dict)}.')\n",
    "        print(f'The type of model.state_dict() is {type(model.state_dict())}.')\n",
    "\n",
    "        for name,param in model.named_parameters():\n",
    "          if name in pretrained_dict.keys():\n",
    "              param.requires_grad = False\n",
    "          else :\n",
    "              param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in pretrained_dict are : \\n {pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in unmatched_pretrained_dict are : \\n {unmatched_pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:39.562181Z",
     "iopub.status.busy": "2024-12-15T07:57:39.561843Z",
     "iopub.status.idle": "2024-12-15T07:57:39.568394Z",
     "shell.execute_reply": "2024-12-15T07:57:39.567522Z",
     "shell.execute_reply.started": "2024-12-15T07:57:39.562151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class for loading texture and roughness data\n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, texture_paths, roughness_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texture_paths (list): List of paths to texture images.\n",
    "            roughness_paths (list): List of paths to roughness images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on both input and target images.\n",
    "        \"\"\"\n",
    "        self.texture_paths = texture_paths\n",
    "        self.roughness_paths = roughness_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texture_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        texture = Image.open(self.texture_paths[idx]).convert(\"RGB\")\n",
    "        roughness = Image.open(self.roughness_paths[idx]).convert(\"L\")\n",
    "\n",
    "        # Apply transforms if defined\n",
    "        if self.transform:\n",
    "            texture = self.transform(texture)\n",
    "            roughness = self.transform(roughness)\n",
    "\n",
    "\n",
    "        return {\"input\": texture, \"target\": roughness}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Texture and Roughness Paths from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:43.155192Z",
     "iopub.status.busy": "2024-12-15T07:57:43.154549Z",
     "iopub.status.idle": "2024-12-15T07:57:43.771009Z",
     "shell.execute_reply": "2024-12-15T07:57:43.77002Z",
     "shell.execute_reply.started": "2024-12-15T07:57:43.155159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the directory containing your CSV files\n",
    "csv_directory = \"/kaggle/input/texture-allpathds\"  # Replace with your directory path\n",
    "\n",
    "# Initialize lists to store paths\n",
    "texture_paths = []\n",
    "roughness_paths = []\n",
    "\n",
    "# Iterate over all CSV files in the directory\n",
    "for file_name in os.listdir(csv_directory):\n",
    "    if file_name.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(csv_directory, file_name)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract columns and append to the lists\n",
    "        if \"Texture\" in df.columns and \"Roughness\" in df.columns:\n",
    "            texture_paths.extend(df[\"Texture\"].dropna().tolist())\n",
    "            roughness_paths.extend(df[\"Roughness\"].dropna().tolist())\n",
    "        else:\n",
    "            print(f\"Warning: File {file_name} does not contain 'texture' and 'Roughness' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:21.307418Z",
     "iopub.status.busy": "2024-12-15T08:00:21.307037Z",
     "iopub.status.idle": "2024-12-15T08:00:21.352863Z",
     "shell.execute_reply": "2024-12-15T08:00:21.351919Z",
     "shell.execute_reply.started": "2024-12-15T08:00:21.307385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_textures, test_textures, train_roughnesses, test_roughnesses = train_test_split(\n",
    "    texture_paths, roughness_paths, test_size=0.01, random_state=42\n",
    ")\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = TextureDataset(train_textures, train_roughnesses, transform=transform)\n",
    "test_dataset = TextureDataset(test_textures, test_roughnesses, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:22.56483Z",
     "iopub.status.busy": "2024-12-15T08:00:22.564465Z",
     "iopub.status.idle": "2024-12-15T08:00:26.074588Z",
     "shell.execute_reply": "2024-12-15T08:00:26.073598Z",
     "shell.execute_reply.started": "2024-12-15T08:00:22.564784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = convnext_base(True, in_22k=False).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:32.893076Z",
     "iopub.status.busy": "2024-12-15T08:00:32.892693Z",
     "iopub.status.idle": "2024-12-15T08:00:32.898754Z",
     "shell.execute_reply": "2024-12-15T08:00:32.897854Z",
     "shell.execute_reply.started": "2024-12-15T08:00:32.893044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set optimizer\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom berHu Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:36.575026Z",
     "iopub.status.busy": "2024-12-15T08:00:36.574325Z",
     "iopub.status.idle": "2024-12-15T08:00:36.58015Z",
     "shell.execute_reply": "2024-12-15T08:00:36.57924Z",
     "shell.execute_reply.started": "2024-12-15T08:00:36.574994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class berHuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(berHuLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(\"Predicted and target images must have the same dimensions.\")\n",
    "\n",
    "        error = torch.abs(pred - target)\n",
    "\n",
    "        c = 0.2 * torch.max(error)\n",
    "\n",
    "        loss = torch.where(\n",
    "            error <= c,\n",
    "            error,\n",
    "            (error ** 2 + c ** 2) / (2 * c)\n",
    "        )\n",
    "\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:36.861278Z",
     "iopub.status.busy": "2024-12-15T08:00:36.860426Z",
     "iopub.status.idle": "2024-12-15T08:00:36.864741Z",
     "shell.execute_reply": "2024-12-15T08:00:36.863869Z",
     "shell.execute_reply.started": "2024-12-15T08:00:36.861242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the berHuLoss\n",
    "criterion = berHuLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging into Weights & Biases (W&B) Using a User Secret API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:46.560557Z",
     "iopub.status.busy": "2024-12-15T08:00:46.560236Z",
     "iopub.status.idle": "2024-12-15T08:00:46.854101Z",
     "shell.execute_reply": "2024-12-15T08:00:46.853211Z",
     "shell.execute_reply.started": "2024-12-15T08:00:46.560533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a generic user secret\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    " \n",
    "#Login to W&B using the retrieved API key\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Uploading a PyTorch Model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)\n",
    "\n",
    "repo_name = \"ConvNeXt_with_skip_connections\"\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "def save_to_huggingface(model):\n",
    "    # Save the model to a .pth file\n",
    "    save_path = \"ConvNeXt_with_skip_connections.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved locally to {save_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj=save_path,\n",
    "        path_in_repo=save_path,\n",
    "        repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "        token=hf_token  # Using the token from environment variable\n",
    "    )\n",
    "    print(f\"Model uploaded to Hugging Face Hub: https://huggingface.co/prakanda/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device0=\"cuda:0\"\n",
    "num_epochs = 1\n",
    "wandb.init(project=\"roughness-estimation-ConvNeXt-with-skip-connections\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "\n",
    "# Move the model to the GPU before the training loop\n",
    "model.to(device0) \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Move inputs, targets, and camera_intrinsic to the device\n",
    "        inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "        targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted_roughness = outputs[-1]\n",
    "\n",
    "        # Use the berHuLoss criterion for roughness loss calculation\n",
    "        loss = criterion(predicted_roughness, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log progress every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        wandb.log({\"epoch\": epoch + 1, \"batch_loss\": loss.item()})\n",
    "\n",
    "        # Save the model every 1000 batches\n",
    "        if batch_idx != 0 and batch_idx % 1000 == 0:\n",
    "            print(\"Saving model\")\n",
    "            save_to_huggingface(model)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    average_loss_per_epoch = running_loss / len(train_dataloader)\n",
    "    wandb.log({\"epoch\": epoch + 1, \"average_loss_per_epoch\": average_loss_per_epoch})\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss_per_epoch:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:01:04.455855Z",
     "iopub.status.busy": "2024-12-15T08:01:04.455137Z",
     "iopub.status.idle": "2024-12-15T08:01:04.724073Z",
     "shell.execute_reply": "2024-12-15T08:01:04.723275Z",
     "shell.execute_reply.started": "2024-12-15T08:01:04.455792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:01:05.207548Z",
     "iopub.status.busy": "2024-12-15T08:01:05.207114Z",
     "iopub.status.idle": "2024-12-15T08:01:05.630961Z",
     "shell.execute_reply": "2024-12-15T08:01:05.630048Z",
     "shell.execute_reply.started": "2024-12-15T08:01:05.207513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the model file from Hugging Face Hub\n",
    "repo_name = \"ConvNeXt_with_skip_connections\"\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "    filename=\"ConvNeXt_with_skip_connections.pth\"\n",
    ")\n",
    "print(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and load the state_dict\n",
    "\n",
    "model.load_state_dict(torch.load(downloaded_file),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:03:53.866267Z",
     "iopub.status.busy": "2024-12-15T08:03:53.865359Z",
     "iopub.status.idle": "2024-12-15T08:05:00.289726Z",
     "shell.execute_reply": "2024-12-15T08:05:00.288854Z",
     "shell.execute_reply.started": "2024-12-15T08:03:53.866228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device0=\"cuda:0\"\n",
    "wandb.init(project=\"roughness-estimation-ConvNeXt\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "def evaluate(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # Accumulate loss over all batches\n",
    "    total_samples = 0   # Track the number of processed samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader):\n",
    "            # Move inputs and targets to the device\n",
    "            inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "            targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            predicted_roughness = outputs[-1]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted_roughness, targets)\n",
    "\n",
    "            # Accumulate running loss and sample count\n",
    "            running_loss += loss.item() * inputs.size(0)  # Weighted by batch size\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average loss over the dataset\n",
    "        avg_loss = running_loss / total_samples\n",
    "        return avg_loss\n",
    "\n",
    "# Perform evaluation on the test set\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Log the average test loss to W&B\n",
    "wandb.log({\"average_test_loss\": test_loss})\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6211511,
     "sourceId": 10076639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220263,
     "sourceId": 10088214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220624,
     "sourceId": 10088660,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6222017,
     "sourceId": 10090456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256336,
     "sourceId": 10137083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
