{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-15T07:56:12.272901Z",
     "iopub.status.busy": "2024-12-15T07:56:12.272649Z",
     "iopub.status.idle": "2024-12-15T07:56:22.010124Z",
     "shell.execute_reply": "2024-12-15T07:56:22.009278Z",
     "shell.execute_reply.started": "2024-12-15T07:56:12.272876Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning and Switching to the `additional_training` Branch in Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:32.164744Z",
     "iopub.status.busy": "2024-12-15T07:57:32.164195Z",
     "iopub.status.idle": "2024-12-15T07:57:32.170770Z",
     "shell.execute_reply": "2024-12-15T07:57:32.169741Z",
     "shell.execute_reply.started": "2024-12-15T07:57:32.164708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:32.383264Z",
     "iopub.status.busy": "2024-12-15T07:57:32.382936Z",
     "iopub.status.idle": "2024-12-15T07:57:34.236446Z",
     "shell.execute_reply": "2024-12-15T07:57:34.235284Z",
     "shell.execute_reply.started": "2024-12-15T07:57:32.383236Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Texture_training'...\n",
      "remote: Enumerating objects: 436, done.\u001b[K\n",
      "remote: Counting objects: 100% (436/436), done.\u001b[K\n",
      "remote: Compressing objects: 100% (208/208), done.\u001b[K\n",
      "remote: Total 436 (delta 218), reused 427 (delta 210), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (436/436), 1.36 MiB | 8.73 MiB/s, done.\n",
      "Resolving deltas: 100% (218/218), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/prathamadh/Texture_training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:34.238665Z",
     "iopub.status.busy": "2024-12-15T07:57:34.238370Z",
     "iopub.status.idle": "2024-12-15T07:57:34.244641Z",
     "shell.execute_reply": "2024-12-15T07:57:34.243790Z",
     "shell.execute_reply.started": "2024-12-15T07:57:34.238638Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Texture_training\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Texture_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:34.245944Z",
     "iopub.status.busy": "2024-12-15T07:57:34.245680Z",
     "iopub.status.idle": "2024-12-15T07:57:36.586140Z",
     "shell.execute_reply": "2024-12-15T07:57:36.585224Z",
     "shell.execute_reply.started": "2024-12-15T07:57:34.245920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'additional_training' set up to track remote branch 'additional_training' from 'origin'.\n",
      "Switched to a new branch 'additional_training'\n"
     ]
    }
   ],
   "source": [
    "! git fetch origin\n",
    "! git checkout additional_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:36.588410Z",
     "iopub.status.busy": "2024-12-15T07:57:36.588102Z",
     "iopub.status.idle": "2024-12-15T07:57:37.592475Z",
     "shell.execute_reply": "2024-12-15T07:57:37.591583Z",
     "shell.execute_reply.started": "2024-12-15T07:57:36.588383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32madditional_training\u001b[m\n",
      "  main\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:37.594117Z",
     "iopub.status.busy": "2024-12-15T07:57:37.593790Z",
     "iopub.status.idle": "2024-12-15T07:57:37.600633Z",
     "shell.execute_reply": "2024-12-15T07:57:37.599714Z",
     "shell.execute_reply.started": "2024-12-15T07:57:37.594089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
    "from huggingface_hub import login\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup and Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:37.601885Z",
     "iopub.status.busy": "2024-12-15T07:57:37.601618Z",
     "iopub.status.idle": "2024-12-15T07:57:37.618332Z",
     "shell.execute_reply": "2024-12-15T07:57:37.617677Z",
     "shell.execute_reply.started": "2024-12-15T07:57:37.601861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeconvNet(nn.Module):\n",
    "    def __init__(self, in_channels, target_height, target_width):\n",
    "        super(DeconvNet, self).__init__()\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "        # Deconvolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels, 512, kernel_size=4, stride=2, padding=1)  # (2x upsample)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)          # (2x upsample)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)          # (2x upsample)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)           # (2x upsample)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1)             # (2x upsample)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "\n",
    "        # Resize to match the target dimensions\n",
    "        x = nn.functional.interpolate(x, size=(self.target_height, self.target_width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:37.698677Z",
     "iopub.status.busy": "2024-12-15T07:57:37.698271Z",
     "iopub.status.idle": "2024-12-15T07:57:37.731418Z",
     "shell.execute_reply": "2024-12-15T07:57:37.730553Z",
     "shell.execute_reply.started": "2024-12-15T07:57:37.698646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, out_chans=1,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 **kwargs,):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Output head for roughness map prediction\n",
    "        self.deconv = DeconvNet(in_channels= dims[-1], target_height=600, target_width=600)\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        features = []\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "            features.append(x)\n",
    "\n",
    "        x = self.deconv(x)\n",
    "        features.append(x)\n",
    "\n",
    "        return features\n",
    "        # return features # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.forward_features(x)\n",
    "        #x = self.head(x)\n",
    "        features = self.forward_features(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "\n",
    "            return x\n",
    "\n",
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "def convnext_tiny(pretrained=True,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        # url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
    "        # checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict:\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "    return model\n",
    "\n",
    "def convnext_small(pretrained=True,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        # url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']\n",
    "        # checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict:\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "    return model\n",
    "\n",
    "def convnext_base(pretrained=True, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
    "    if pretrained:\n",
    "        # checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # print(model_dict)\n",
    "        # print(checkpoint)\n",
    "\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict.keys():\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        print(f'The type of checkpoint is {type(checkpoint)}.')\n",
    "        print(f'The type of model.state_dict is {type(model.state_dict)}.')\n",
    "        print(f'The type of model.state_dict() is {type(model.state_dict())}.')\n",
    "\n",
    "        for name,param in model.named_parameters():\n",
    "          if name in pretrained_dict.keys():\n",
    "              param.requires_grad = False\n",
    "          else :\n",
    "              param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in pretrained_dict are : \\n {pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in unmatched_pretrained_dict are : \\n {unmatched_pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def convnext_large(pretrained=True, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
    "    if pretrained:\n",
    "        # checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict:\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "    return model\n",
    "\n",
    "def convnext_xlarge(pretrained=True, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
    "    if pretrained:\n",
    "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
    "        checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        #url = model_urls['convnext_xlarge_22k']\n",
    "        #checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict:\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:39.562181Z",
     "iopub.status.busy": "2024-12-15T07:57:39.561843Z",
     "iopub.status.idle": "2024-12-15T07:57:39.568394Z",
     "shell.execute_reply": "2024-12-15T07:57:39.567522Z",
     "shell.execute_reply.started": "2024-12-15T07:57:39.562151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class for loading texture and roughness data\n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, texture_paths, roughness_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texture_paths (list): List of paths to texture images.\n",
    "            roughness_paths (list): List of paths to roughness images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on both input and target images.\n",
    "        \"\"\"\n",
    "        self.texture_paths = texture_paths\n",
    "        self.roughness_paths = roughness_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texture_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        texture = Image.open(self.texture_paths[idx]).convert(\"RGB\")\n",
    "        roughness = Image.open(self.roughness_paths[idx]).convert(\"L\")\n",
    "\n",
    "        # Apply transforms if defined\n",
    "        if self.transform:\n",
    "            texture = self.transform(texture)\n",
    "            roughness = self.transform(roughness)\n",
    "\n",
    "\n",
    "        return {\"input\": texture, \"target\": roughness}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Texture and Roughness Paths from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:57:43.155192Z",
     "iopub.status.busy": "2024-12-15T07:57:43.154549Z",
     "iopub.status.idle": "2024-12-15T07:57:43.771009Z",
     "shell.execute_reply": "2024-12-15T07:57:43.770020Z",
     "shell.execute_reply.started": "2024-12-15T07:57:43.155159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the directory containing your CSV files\n",
    "csv_directory = \"/kaggle/input/texture-allpathds\"  # Replace with your directory path\n",
    "\n",
    "# Initialize lists to store paths\n",
    "texture_paths = []\n",
    "roughness_paths = []\n",
    "\n",
    "# Iterate over all CSV files in the directory\n",
    "for file_name in os.listdir(csv_directory):\n",
    "    if file_name.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(csv_directory, file_name)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract columns and append to the lists\n",
    "        if \"Texture\" in df.columns and \"Roughness\" in df.columns:\n",
    "            texture_paths.extend(df[\"Texture\"].dropna().tolist())\n",
    "            roughness_paths.extend(df[\"Roughness\"].dropna().tolist())\n",
    "        else:\n",
    "            print(f\"Warning: File {file_name} does not contain 'texture' and 'Roughness' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:21.307418Z",
     "iopub.status.busy": "2024-12-15T08:00:21.307037Z",
     "iopub.status.idle": "2024-12-15T08:00:21.352863Z",
     "shell.execute_reply": "2024-12-15T08:00:21.351919Z",
     "shell.execute_reply.started": "2024-12-15T08:00:21.307385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_textures, test_textures, train_roughnesses, test_roughnesses = train_test_split(\n",
    "    texture_paths, roughness_paths, test_size=0.01, random_state=42\n",
    ")\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = TextureDataset(train_textures, train_roughnesses, transform=transform)\n",
    "test_dataset = TextureDataset(test_textures, test_roughnesses, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:22.564830Z",
     "iopub.status.busy": "2024-12-15T08:00:22.564465Z",
     "iopub.status.idle": "2024-12-15T08:00:26.074588Z",
     "shell.execute_reply": "2024-12-15T08:00:26.073598Z",
     "shell.execute_reply.started": "2024-12-15T08:00:22.564784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n",
      "100%|██████████| 338M/338M [00:01<00:00, 310MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of checkpoint is <class 'dict'>.\n",
      "The type of model.state_dict is <class 'method'>.\n",
      "The type of model.state_dict() is <class 'collections.OrderedDict'>.\n",
      "\n",
      "\n",
      "The keys in pretrained_dict are : \n",
      " dict_keys(['downsample_layers.0.0.weight', 'downsample_layers.0.0.bias', 'downsample_layers.0.1.weight', 'downsample_layers.0.1.bias', 'downsample_layers.1.0.weight', 'downsample_layers.1.0.bias', 'downsample_layers.1.1.weight', 'downsample_layers.1.1.bias', 'downsample_layers.2.0.weight', 'downsample_layers.2.0.bias', 'downsample_layers.2.1.weight', 'downsample_layers.2.1.bias', 'downsample_layers.3.0.weight', 'downsample_layers.3.0.bias', 'downsample_layers.3.1.weight', 'downsample_layers.3.1.bias', 'stages.0.0.gamma', 'stages.0.0.dwconv.weight', 'stages.0.0.dwconv.bias', 'stages.0.0.norm.weight', 'stages.0.0.norm.bias', 'stages.0.0.pwconv1.weight', 'stages.0.0.pwconv1.bias', 'stages.0.0.pwconv2.weight', 'stages.0.0.pwconv2.bias', 'stages.0.1.gamma', 'stages.0.1.dwconv.weight', 'stages.0.1.dwconv.bias', 'stages.0.1.norm.weight', 'stages.0.1.norm.bias', 'stages.0.1.pwconv1.weight', 'stages.0.1.pwconv1.bias', 'stages.0.1.pwconv2.weight', 'stages.0.1.pwconv2.bias', 'stages.0.2.gamma', 'stages.0.2.dwconv.weight', 'stages.0.2.dwconv.bias', 'stages.0.2.norm.weight', 'stages.0.2.norm.bias', 'stages.0.2.pwconv1.weight', 'stages.0.2.pwconv1.bias', 'stages.0.2.pwconv2.weight', 'stages.0.2.pwconv2.bias', 'stages.1.0.gamma', 'stages.1.0.dwconv.weight', 'stages.1.0.dwconv.bias', 'stages.1.0.norm.weight', 'stages.1.0.norm.bias', 'stages.1.0.pwconv1.weight', 'stages.1.0.pwconv1.bias', 'stages.1.0.pwconv2.weight', 'stages.1.0.pwconv2.bias', 'stages.1.1.gamma', 'stages.1.1.dwconv.weight', 'stages.1.1.dwconv.bias', 'stages.1.1.norm.weight', 'stages.1.1.norm.bias', 'stages.1.1.pwconv1.weight', 'stages.1.1.pwconv1.bias', 'stages.1.1.pwconv2.weight', 'stages.1.1.pwconv2.bias', 'stages.1.2.gamma', 'stages.1.2.dwconv.weight', 'stages.1.2.dwconv.bias', 'stages.1.2.norm.weight', 'stages.1.2.norm.bias', 'stages.1.2.pwconv1.weight', 'stages.1.2.pwconv1.bias', 'stages.1.2.pwconv2.weight', 'stages.1.2.pwconv2.bias', 'stages.2.0.gamma', 'stages.2.0.dwconv.weight', 'stages.2.0.dwconv.bias', 'stages.2.0.norm.weight', 'stages.2.0.norm.bias', 'stages.2.0.pwconv1.weight', 'stages.2.0.pwconv1.bias', 'stages.2.0.pwconv2.weight', 'stages.2.0.pwconv2.bias', 'stages.2.1.gamma', 'stages.2.1.dwconv.weight', 'stages.2.1.dwconv.bias', 'stages.2.1.norm.weight', 'stages.2.1.norm.bias', 'stages.2.1.pwconv1.weight', 'stages.2.1.pwconv1.bias', 'stages.2.1.pwconv2.weight', 'stages.2.1.pwconv2.bias', 'stages.2.2.gamma', 'stages.2.2.dwconv.weight', 'stages.2.2.dwconv.bias', 'stages.2.2.norm.weight', 'stages.2.2.norm.bias', 'stages.2.2.pwconv1.weight', 'stages.2.2.pwconv1.bias', 'stages.2.2.pwconv2.weight', 'stages.2.2.pwconv2.bias', 'stages.2.3.gamma', 'stages.2.3.dwconv.weight', 'stages.2.3.dwconv.bias', 'stages.2.3.norm.weight', 'stages.2.3.norm.bias', 'stages.2.3.pwconv1.weight', 'stages.2.3.pwconv1.bias', 'stages.2.3.pwconv2.weight', 'stages.2.3.pwconv2.bias', 'stages.2.4.gamma', 'stages.2.4.dwconv.weight', 'stages.2.4.dwconv.bias', 'stages.2.4.norm.weight', 'stages.2.4.norm.bias', 'stages.2.4.pwconv1.weight', 'stages.2.4.pwconv1.bias', 'stages.2.4.pwconv2.weight', 'stages.2.4.pwconv2.bias', 'stages.2.5.gamma', 'stages.2.5.dwconv.weight', 'stages.2.5.dwconv.bias', 'stages.2.5.norm.weight', 'stages.2.5.norm.bias', 'stages.2.5.pwconv1.weight', 'stages.2.5.pwconv1.bias', 'stages.2.5.pwconv2.weight', 'stages.2.5.pwconv2.bias', 'stages.2.6.gamma', 'stages.2.6.dwconv.weight', 'stages.2.6.dwconv.bias', 'stages.2.6.norm.weight', 'stages.2.6.norm.bias', 'stages.2.6.pwconv1.weight', 'stages.2.6.pwconv1.bias', 'stages.2.6.pwconv2.weight', 'stages.2.6.pwconv2.bias', 'stages.2.7.gamma', 'stages.2.7.dwconv.weight', 'stages.2.7.dwconv.bias', 'stages.2.7.norm.weight', 'stages.2.7.norm.bias', 'stages.2.7.pwconv1.weight', 'stages.2.7.pwconv1.bias', 'stages.2.7.pwconv2.weight', 'stages.2.7.pwconv2.bias', 'stages.2.8.gamma', 'stages.2.8.dwconv.weight', 'stages.2.8.dwconv.bias', 'stages.2.8.norm.weight', 'stages.2.8.norm.bias', 'stages.2.8.pwconv1.weight', 'stages.2.8.pwconv1.bias', 'stages.2.8.pwconv2.weight', 'stages.2.8.pwconv2.bias', 'stages.2.9.gamma', 'stages.2.9.dwconv.weight', 'stages.2.9.dwconv.bias', 'stages.2.9.norm.weight', 'stages.2.9.norm.bias', 'stages.2.9.pwconv1.weight', 'stages.2.9.pwconv1.bias', 'stages.2.9.pwconv2.weight', 'stages.2.9.pwconv2.bias', 'stages.2.10.gamma', 'stages.2.10.dwconv.weight', 'stages.2.10.dwconv.bias', 'stages.2.10.norm.weight', 'stages.2.10.norm.bias', 'stages.2.10.pwconv1.weight', 'stages.2.10.pwconv1.bias', 'stages.2.10.pwconv2.weight', 'stages.2.10.pwconv2.bias', 'stages.2.11.gamma', 'stages.2.11.dwconv.weight', 'stages.2.11.dwconv.bias', 'stages.2.11.norm.weight', 'stages.2.11.norm.bias', 'stages.2.11.pwconv1.weight', 'stages.2.11.pwconv1.bias', 'stages.2.11.pwconv2.weight', 'stages.2.11.pwconv2.bias', 'stages.2.12.gamma', 'stages.2.12.dwconv.weight', 'stages.2.12.dwconv.bias', 'stages.2.12.norm.weight', 'stages.2.12.norm.bias', 'stages.2.12.pwconv1.weight', 'stages.2.12.pwconv1.bias', 'stages.2.12.pwconv2.weight', 'stages.2.12.pwconv2.bias', 'stages.2.13.gamma', 'stages.2.13.dwconv.weight', 'stages.2.13.dwconv.bias', 'stages.2.13.norm.weight', 'stages.2.13.norm.bias', 'stages.2.13.pwconv1.weight', 'stages.2.13.pwconv1.bias', 'stages.2.13.pwconv2.weight', 'stages.2.13.pwconv2.bias', 'stages.2.14.gamma', 'stages.2.14.dwconv.weight', 'stages.2.14.dwconv.bias', 'stages.2.14.norm.weight', 'stages.2.14.norm.bias', 'stages.2.14.pwconv1.weight', 'stages.2.14.pwconv1.bias', 'stages.2.14.pwconv2.weight', 'stages.2.14.pwconv2.bias', 'stages.2.15.gamma', 'stages.2.15.dwconv.weight', 'stages.2.15.dwconv.bias', 'stages.2.15.norm.weight', 'stages.2.15.norm.bias', 'stages.2.15.pwconv1.weight', 'stages.2.15.pwconv1.bias', 'stages.2.15.pwconv2.weight', 'stages.2.15.pwconv2.bias', 'stages.2.16.gamma', 'stages.2.16.dwconv.weight', 'stages.2.16.dwconv.bias', 'stages.2.16.norm.weight', 'stages.2.16.norm.bias', 'stages.2.16.pwconv1.weight', 'stages.2.16.pwconv1.bias', 'stages.2.16.pwconv2.weight', 'stages.2.16.pwconv2.bias', 'stages.2.17.gamma', 'stages.2.17.dwconv.weight', 'stages.2.17.dwconv.bias', 'stages.2.17.norm.weight', 'stages.2.17.norm.bias', 'stages.2.17.pwconv1.weight', 'stages.2.17.pwconv1.bias', 'stages.2.17.pwconv2.weight', 'stages.2.17.pwconv2.bias', 'stages.2.18.gamma', 'stages.2.18.dwconv.weight', 'stages.2.18.dwconv.bias', 'stages.2.18.norm.weight', 'stages.2.18.norm.bias', 'stages.2.18.pwconv1.weight', 'stages.2.18.pwconv1.bias', 'stages.2.18.pwconv2.weight', 'stages.2.18.pwconv2.bias', 'stages.2.19.gamma', 'stages.2.19.dwconv.weight', 'stages.2.19.dwconv.bias', 'stages.2.19.norm.weight', 'stages.2.19.norm.bias', 'stages.2.19.pwconv1.weight', 'stages.2.19.pwconv1.bias', 'stages.2.19.pwconv2.weight', 'stages.2.19.pwconv2.bias', 'stages.2.20.gamma', 'stages.2.20.dwconv.weight', 'stages.2.20.dwconv.bias', 'stages.2.20.norm.weight', 'stages.2.20.norm.bias', 'stages.2.20.pwconv1.weight', 'stages.2.20.pwconv1.bias', 'stages.2.20.pwconv2.weight', 'stages.2.20.pwconv2.bias', 'stages.2.21.gamma', 'stages.2.21.dwconv.weight', 'stages.2.21.dwconv.bias', 'stages.2.21.norm.weight', 'stages.2.21.norm.bias', 'stages.2.21.pwconv1.weight', 'stages.2.21.pwconv1.bias', 'stages.2.21.pwconv2.weight', 'stages.2.21.pwconv2.bias', 'stages.2.22.gamma', 'stages.2.22.dwconv.weight', 'stages.2.22.dwconv.bias', 'stages.2.22.norm.weight', 'stages.2.22.norm.bias', 'stages.2.22.pwconv1.weight', 'stages.2.22.pwconv1.bias', 'stages.2.22.pwconv2.weight', 'stages.2.22.pwconv2.bias', 'stages.2.23.gamma', 'stages.2.23.dwconv.weight', 'stages.2.23.dwconv.bias', 'stages.2.23.norm.weight', 'stages.2.23.norm.bias', 'stages.2.23.pwconv1.weight', 'stages.2.23.pwconv1.bias', 'stages.2.23.pwconv2.weight', 'stages.2.23.pwconv2.bias', 'stages.2.24.gamma', 'stages.2.24.dwconv.weight', 'stages.2.24.dwconv.bias', 'stages.2.24.norm.weight', 'stages.2.24.norm.bias', 'stages.2.24.pwconv1.weight', 'stages.2.24.pwconv1.bias', 'stages.2.24.pwconv2.weight', 'stages.2.24.pwconv2.bias', 'stages.2.25.gamma', 'stages.2.25.dwconv.weight', 'stages.2.25.dwconv.bias', 'stages.2.25.norm.weight', 'stages.2.25.norm.bias', 'stages.2.25.pwconv1.weight', 'stages.2.25.pwconv1.bias', 'stages.2.25.pwconv2.weight', 'stages.2.25.pwconv2.bias', 'stages.2.26.gamma', 'stages.2.26.dwconv.weight', 'stages.2.26.dwconv.bias', 'stages.2.26.norm.weight', 'stages.2.26.norm.bias', 'stages.2.26.pwconv1.weight', 'stages.2.26.pwconv1.bias', 'stages.2.26.pwconv2.weight', 'stages.2.26.pwconv2.bias', 'stages.3.0.gamma', 'stages.3.0.dwconv.weight', 'stages.3.0.dwconv.bias', 'stages.3.0.norm.weight', 'stages.3.0.norm.bias', 'stages.3.0.pwconv1.weight', 'stages.3.0.pwconv1.bias', 'stages.3.0.pwconv2.weight', 'stages.3.0.pwconv2.bias', 'stages.3.1.gamma', 'stages.3.1.dwconv.weight', 'stages.3.1.dwconv.bias', 'stages.3.1.norm.weight', 'stages.3.1.norm.bias', 'stages.3.1.pwconv1.weight', 'stages.3.1.pwconv1.bias', 'stages.3.1.pwconv2.weight', 'stages.3.1.pwconv2.bias', 'stages.3.2.gamma', 'stages.3.2.dwconv.weight', 'stages.3.2.dwconv.bias', 'stages.3.2.norm.weight', 'stages.3.2.norm.bias', 'stages.3.2.pwconv1.weight', 'stages.3.2.pwconv1.bias', 'stages.3.2.pwconv2.weight', 'stages.3.2.pwconv2.bias'])\n",
      "\n",
      "\n",
      "The keys in unmatched_pretrained_dict are : \n",
      " dict_keys(['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])\n",
      "\n",
      "\n",
      "Successfully loaded pretrained 340 paras, and 4 paras are unmatched.\n",
      "\n",
      "\n",
      "Unmatched pretrained paras are: dict_keys(['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = convnext_base(True, in_22k=False).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:32.893076Z",
     "iopub.status.busy": "2024-12-15T08:00:32.892693Z",
     "iopub.status.idle": "2024-12-15T08:00:32.898754Z",
     "shell.execute_reply": "2024-12-15T08:00:32.897854Z",
     "shell.execute_reply.started": "2024-12-15T08:00:32.893044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set optimizer\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom berHu Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:36.575026Z",
     "iopub.status.busy": "2024-12-15T08:00:36.574325Z",
     "iopub.status.idle": "2024-12-15T08:00:36.580150Z",
     "shell.execute_reply": "2024-12-15T08:00:36.579240Z",
     "shell.execute_reply.started": "2024-12-15T08:00:36.574994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class berHuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(berHuLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(\"Predicted and target images must have the same dimensions.\")\n",
    "\n",
    "        error = torch.abs(pred - target)\n",
    "\n",
    "        c = 0.2 * torch.max(error)\n",
    "\n",
    "        loss = torch.where(\n",
    "            error <= c,\n",
    "            error,\n",
    "            (error ** 2 + c ** 2) / (2 * c)\n",
    "        )\n",
    "\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:36.861278Z",
     "iopub.status.busy": "2024-12-15T08:00:36.860426Z",
     "iopub.status.idle": "2024-12-15T08:00:36.864741Z",
     "shell.execute_reply": "2024-12-15T08:00:36.863869Z",
     "shell.execute_reply.started": "2024-12-15T08:00:36.861242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the berHuLoss\n",
    "criterion = berHuLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging into Weights & Biases (W&B) Using a User Secret API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:00:46.560557Z",
     "iopub.status.busy": "2024-12-15T08:00:46.560236Z",
     "iopub.status.idle": "2024-12-15T08:00:46.854101Z",
     "shell.execute_reply": "2024-12-15T08:00:46.853211Z",
     "shell.execute_reply.started": "2024-12-15T08:00:46.560533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprakandabhandari\u001b[0m (\u001b[33mprakandabhandari-tribhuvan-university-institute-of-engin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a generic user secret\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    " \n",
    "#Login to W&B using the retrieved API key\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Uploading a PyTorch Model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)\n",
    "\n",
    "repo_name = \"ConvNeXt_roughness_model\"\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "def save_to_huggingface(model):\n",
    "    # Save the model to a .pth file\n",
    "    save_path = \"ConvNeXt_roughness_model.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved locally to {save_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj=save_path,\n",
    "        path_in_repo=save_path,\n",
    "        repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "        token=hf_token  # Using the token from environment variable\n",
    "    )\n",
    "    print(f\"Model uploaded to Hugging Face Hub: https://huggingface.co/prakanda/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device0=\"cuda:0\"\n",
    "num_epochs = 1\n",
    "wandb.init(project=\"roughness-estimation-ConvNeXt\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "\n",
    "# Move the model to the GPU before the training loop\n",
    "model.to(device0) \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Move inputs, targets, and camera_intrinsic to the device\n",
    "        inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "        targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted_roughness = outputs[-1]\n",
    "\n",
    "        # Use the berHuLoss criterion for roughness loss calculation\n",
    "        loss = criterion(predicted_roughness, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log progress every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        wandb.log({\"epoch\": epoch + 1, \"batch_loss\": loss.item()})\n",
    "\n",
    "        # Save the model every 1000 batches\n",
    "        if batch_idx != 0 and batch_idx % 1000 == 0:\n",
    "            print(\"Saving model\")\n",
    "            save_to_huggingface(model)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    average_loss_per_epoch = running_loss / len(train_dataloader)\n",
    "    wandb.log({\"epoch\": epoch + 1, \"average_loss_per_epoch\": average_loss_per_epoch})\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss_per_epoch:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:01:04.455855Z",
     "iopub.status.busy": "2024-12-15T08:01:04.455137Z",
     "iopub.status.idle": "2024-12-15T08:01:04.724073Z",
     "shell.execute_reply": "2024-12-15T08:01:04.723275Z",
     "shell.execute_reply.started": "2024-12-15T08:01:04.455792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:01:05.207548Z",
     "iopub.status.busy": "2024-12-15T08:01:05.207114Z",
     "iopub.status.idle": "2024-12-15T08:01:05.630961Z",
     "shell.execute_reply": "2024-12-15T08:01:05.630048Z",
     "shell.execute_reply.started": "2024-12-15T08:01:05.207513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded from Hugging Face Hub: /root/.cache/huggingface/hub/models--prakanda--ConvNeXt_roughness_model/snapshots/15327197e2c54c7a4351f553595169aa6d926f16/ConvNeXt_roughness_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3283234082.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(downloaded_file),strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the model file from Hugging Face Hub\n",
    "repo_name = \"ConvNeXt_roughness_model\"\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "    filename=\"ConvNeXt_roughness_model.pth\"\n",
    ")\n",
    "print(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and load the state_dict\n",
    "\n",
    "model.load_state_dict(torch.load(downloaded_file),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T08:03:53.866267Z",
     "iopub.status.busy": "2024-12-15T08:03:53.865359Z",
     "iopub.status.idle": "2024-12-15T08:05:00.289726Z",
     "shell.execute_reply": "2024-12-15T08:05:00.288854Z",
     "shell.execute_reply.started": "2024-12-15T08:03:53.866228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/Texture_training/wandb/run-20241215_080353-0r2du0wd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt/runs/0r2du0wd' target=\"_blank\">charmed-darkness-2</a></strong> to <a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt' target=\"_blank\">https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt/runs/0r2du0wd' target=\"_blank\">https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt/runs/0r2du0wd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0278\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_loss</td><td>0.0278</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-darkness-2</strong> at: <a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt/runs/0r2du0wd' target=\"_blank\">https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt/runs/0r2du0wd</a><br/> View project at: <a href='https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt' target=\"_blank\">https://wandb.ai/prakandabhandari-tribhuvan-university-institute-of-engin/roughness-estimation-ConvNeXt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241215_080353-0r2du0wd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "device0=\"cuda:0\"\n",
    "wandb.init(project=\"roughness-estimation-ConvNeXt\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "def evaluate(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # Accumulate loss over all batches\n",
    "    total_samples = 0   # Track the number of processed samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader):\n",
    "            # Move inputs and targets to the device\n",
    "            inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "            targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            predicted_roughness = outputs[-1]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted_roughness, targets)\n",
    "\n",
    "            # Accumulate running loss and sample count\n",
    "            running_loss += loss.item() * inputs.size(0)  # Weighted by batch size\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average loss over the dataset\n",
    "        avg_loss = running_loss / total_samples\n",
    "        return avg_loss\n",
    "\n",
    "# Perform evaluation on the test set\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Log the average test loss to W&B\n",
    "wandb.log({\"average_test_loss\": test_loss})\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6211511,
     "sourceId": 10076639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220263,
     "sourceId": 10088214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220624,
     "sourceId": 10088660,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6222017,
     "sourceId": 10090456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256336,
     "sourceId": 10137083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
