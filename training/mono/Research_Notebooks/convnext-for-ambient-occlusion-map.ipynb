{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T03:29:49.916985Z",
     "iopub.status.busy": "2024-12-20T03:29:49.916389Z",
     "iopub.status.idle": "2024-12-20T03:30:07.469424Z",
     "shell.execute_reply": "2024-12-20T03:30:07.468391Z",
     "shell.execute_reply.started": "2024-12-20T03:29:49.916955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install wandb\n",
    "! pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:07.471841Z",
     "iopub.status.busy": "2024-12-20T03:30:07.471522Z",
     "iopub.status.idle": "2024-12-20T03:30:13.957505Z",
     "shell.execute_reply": "2024-12-20T03:30:13.956661Z",
     "shell.execute_reply.started": "2024-12-20T03:30:07.471788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\n",
    "from huggingface_hub import login\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "from huggingface_hub import hf_hub_download\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup and Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:13.959329Z",
     "iopub.status.busy": "2024-12-20T03:30:13.958766Z",
     "iopub.status.idle": "2024-12-20T03:30:13.966617Z",
     "shell.execute_reply": "2024-12-20T03:30:13.965851Z",
     "shell.execute_reply.started": "2024-12-20T03:30:13.959300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AOHead(nn.Module):\n",
    "    def __init__(self, in_channels=768, out_channels=1):\n",
    "        super(AOHead, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for further refinement of features\n",
    "        self.conv1 = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Output layer: Single channel for the AO map\n",
    "        self.conv_out = nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # For output normalization to range [0, 1]\n",
    "        \n",
    "        # Upsample to 600x600\n",
    "        self.upsample = nn.Upsample(size=(600, 600), mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the AO head.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input feature map from the ConvNeXt backbone.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The predicted Ambient Occlusion map.\n",
    "        \"\"\"\n",
    "        # Pass through the convolutional layers with ReLU activations\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        \n",
    "        # Final output (predicting AO map)\n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        # Apply sigmoid to get values in the range [0, 1] (for visualizing AO)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        # Upsample to (600, 600)\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:13.967866Z",
     "iopub.status.busy": "2024-12-20T03:30:13.967597Z",
     "iopub.status.idle": "2024-12-20T03:30:13.985281Z",
     "shell.execute_reply": "2024-12-20T03:30:13.984626Z",
     "shell.execute_reply.started": "2024-12-20T03:30:13.967835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:13.987264Z",
     "iopub.status.busy": "2024-12-20T03:30:13.987030Z",
     "iopub.status.idle": "2024-12-20T03:30:14.001975Z",
     "shell.execute_reply": "2024-12-20T03:30:14.001089Z",
     "shell.execute_reply.started": "2024-12-20T03:30:13.987241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, out_chans=1,\n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 **kwargs,):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Output head for ambient map prediction\n",
    "        self.ao_head = AOHead(in_channels= dims[-1], out_channels=1)\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        features = []\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "            features.append(x)\n",
    "\n",
    "        x = self.ao_head(x)\n",
    "        features.append(x)\n",
    "\n",
    "        return features\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.forward_features(x)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:14.003209Z",
     "iopub.status.busy": "2024-12-20T03:30:14.002895Z",
     "iopub.status.idle": "2024-12-20T03:30:14.016357Z",
     "shell.execute_reply": "2024-12-20T03:30:14.015648Z",
     "shell.execute_reply.started": "2024-12-20T03:30:14.003173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:14.017614Z",
     "iopub.status.busy": "2024-12-20T03:30:14.017353Z",
     "iopub.status.idle": "2024-12-20T03:30:14.028209Z",
     "shell.execute_reply": "2024-12-20T03:30:14.027587Z",
     "shell.execute_reply.started": "2024-12-20T03:30:14.017589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:14.029462Z",
     "iopub.status.busy": "2024-12-20T03:30:14.029168Z",
     "iopub.status.idle": "2024-12-20T03:30:14.042909Z",
     "shell.execute_reply": "2024-12-20T03:30:14.042147Z",
     "shell.execute_reply.started": "2024-12-20T03:30:14.029413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convnext_tiny(pretrained=True,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        #checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n",
    "        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {}\n",
    "        unmatched_pretrained_dict = {}\n",
    "        for k, v in checkpoint['model'].items():\n",
    "            if k in model_dict:\n",
    "                pretrained_dict[k] = v\n",
    "            else:\n",
    "                unmatched_pretrained_dict[k] = v\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "\n",
    "        print(f'The type of checkpoint is {type(checkpoint)}.')\n",
    "        print(f'The type of model.state_dict is {type(model.state_dict)}.')\n",
    "        print(f'The type of model.state_dict() is {type(model.state_dict())}.')\n",
    "\n",
    "\n",
    "        \n",
    "        for name,param in model.named_parameters():\n",
    "          if name in pretrained_dict.keys():\n",
    "              param.requires_grad = True\n",
    "          else :\n",
    "              param.requires_grad = True\n",
    "\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in pretrained_dict are : \\n {pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(f'The keys in unmatched_pretrained_dict are : \\n {unmatched_pretrained_dict.keys()}')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print(\n",
    "            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n",
    "            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize:\n",
    "    def __call__(self, image_tensor):\n",
    "        # Apply the given normalization directly to the tensor\n",
    "        normalized_image = torch.where(\n",
    "            image_tensor < 1,  # Condition to normalize only values less than 1.0 condition\n",
    "            torch.clamp((image_tensor - 0.7) / (1.0 - 0.68) * 0.99, min=0),  # Normalize to [0, 0.99] and clip at 0 input \n",
    "            torch.tensor(1.0)  # Keep values of 1.0 intact others\n",
    "        )\n",
    "        \n",
    "        return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:14.044024Z",
     "iopub.status.busy": "2024-12-20T03:30:14.043782Z",
     "iopub.status.idle": "2024-12-20T03:30:14.057114Z",
     "shell.execute_reply": "2024-12-20T03:30:14.056459Z",
     "shell.execute_reply.started": "2024-12-20T03:30:14.044002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class for loading texture and ambient data\n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, texture_paths, ambient_paths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texture_paths (list): List of paths to texture images.\n",
    "            ambient_paths (list): List of paths to ambient images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on both input and target images.\n",
    "        \"\"\"\n",
    "        self.texture_paths = texture_paths\n",
    "        self.ambient_paths = ambient_paths\n",
    "        self.transform_rgb = transforms.Compose([\n",
    "                transforms.Resize((600, 600)),  # Resize images\n",
    "                transforms.ToTensor(),          # Convert images to tensors\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize images (standard values for RGB)\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                transforms.ColorJitter(brightness=0.2),  # Random brightness change\n",
    "            ])\n",
    "            \n",
    "        self.transform_ao = transforms.Compose([\n",
    "                    transforms.Resize((600, 600)), # Resize images\n",
    "                    transforms.ToTensor(), \n",
    "                    CustomNormalize(),  # Apply custom normalization\n",
    "                    transforms.Normalize(mean=[0.5], std=[0.5])  # Standard normalization\n",
    "                ])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texture_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        texture = Image.open(self.texture_paths[idx]).convert(\"RGB\")\n",
    "        ambient = Image.open(self.ambient_paths[idx]).convert(\"L\")\n",
    "\n",
    "        # Apply transformation to texture image (RGB)\n",
    "        if self.transform_rgb:\n",
    "            texture = self.transform_rgb(texture)\n",
    "        # Apply transformation to ambient occlusion image (AO map)\n",
    "        if self.transform_ao:\n",
    "            ambient = self.transform_ao(ambient)\n",
    "\n",
    "\n",
    "        return {\"input\": texture, \"target\": ambient}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Texture and Ambient Paths from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:30:14.058308Z",
     "iopub.status.busy": "2024-12-20T03:30:14.058087Z",
     "iopub.status.idle": "2024-12-20T03:31:34.503523Z",
     "shell.execute_reply": "2024-12-20T03:31:34.502775Z",
     "shell.execute_reply.started": "2024-12-20T03:30:14.058287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Specify the directory containing your CSV files\n",
    "csv_directory = \"/kaggle/input/texture-allpathds\"  # Replace with your directory path\n",
    "\n",
    "# Initialize lists to store paths\n",
    "texture_paths = []\n",
    "ambient_paths = []\n",
    "\n",
    "# Iterate over all CSV files in the directory\n",
    "for file_name in os.listdir(csv_directory):\n",
    "    if file_name.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(csv_directory, file_name)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        df[\"Ambient\"] = df[\"Depth\"].apply(lambda x: x.replace(\"/sa_\", \"/AOsa_\"))\n",
    "\n",
    "        \n",
    "        # Function to replace the path\n",
    "        def replace_path(path):\n",
    "            # Extract the file name\n",
    "            filename = os.path.basename(path)\n",
    "            \n",
    "            # Construct the new path\n",
    "            new_path = f\"/kaggle/input/ambinet-occlusion/{filename}\"\n",
    "            return new_path\n",
    "        \n",
    "        # Apply the function to the column\n",
    "        df[\"Ambient\"] = df[\"Ambient\"].apply(replace_path)\n",
    "\n",
    "        # Check if the paths exist\n",
    "        df = df[df[\"Ambient\"].apply(os.path.exists)]\n",
    "\n",
    "\n",
    "        # Extract columns and append to the lists\n",
    "        if \"Texture\" in df.columns and \"Ambient\" in df.columns:\n",
    "            texture_paths.extend(df[\"Texture\"].dropna().tolist())\n",
    "\n",
    "            \n",
    "            ambient_paths.extend(df[\"Ambient\"].dropna().tolist())\n",
    "        else:\n",
    "            print(f\"Warning: File {file_name} does not contain 'texture' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:34.504666Z",
     "iopub.status.busy": "2024-12-20T03:31:34.504436Z",
     "iopub.status.idle": "2024-12-20T03:31:34.517792Z",
     "shell.execute_reply": "2024-12-20T03:31:34.517171Z",
     "shell.execute_reply.started": "2024-12-20T03:31:34.504642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_textures, test_textures, train_ambient, test_ambient = train_test_split(\n",
    "    texture_paths, ambient_paths, test_size=0.04, random_state=20\n",
    ")\n",
    "\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = TextureDataset(train_textures, train_ambient)\n",
    "test_dataset = TextureDataset(test_textures, test_ambient)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of testing batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:34.518958Z",
     "iopub.status.busy": "2024-12-20T03:31:34.518637Z",
     "iopub.status.idle": "2024-12-20T03:31:37.303671Z",
     "shell.execute_reply": "2024-12-20T03:31:37.302333Z",
     "shell.execute_reply.started": "2024-12-20T03:31:34.518917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = convnext_tiny(True, in_22k=False).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:37.305063Z",
     "iopub.status.busy": "2024-12-20T03:31:37.304732Z",
     "iopub.status.idle": "2024-12-20T03:31:37.312350Z",
     "shell.execute_reply": "2024-12-20T03:31:37.311186Z",
     "shell.execute_reply.started": "2024-12-20T03:31:37.305035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set optimizer\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:37.318113Z",
     "iopub.status.busy": "2024-12-20T03:31:37.317289Z",
     "iopub.status.idle": "2024-12-20T03:31:37.334039Z",
     "shell.execute_reply": "2024-12-20T03:31:37.333034Z",
     "shell.execute_reply.started": "2024-12-20T03:31:37.318069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, ssim_weight=0, l1_weight=0.5, grad_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ssim_weight = ssim_weight\n",
    "        self.l1_weight = l1_weight\n",
    "        self.grad_weight = grad_weight\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        # Ensure shapes match\n",
    "        if predicted.shape != target.shape:\n",
    "            raise ValueError(f\"Shape mismatch: predicted {predicted.shape}, target {target.shape}\")\n",
    "        \n",
    "        # Calculate individual losses\n",
    "        ssim_loss = self.ssim(predicted, target)\n",
    "        l1_loss = self.l1(predicted, target)\n",
    "        grad_loss = self.gradient_loss(predicted, target)\n",
    "        \n",
    "        # Combine the losses with the specified weights\n",
    "        total_loss = self.ssim_weight * ssim_loss + self.l1_weight * l1_loss + self.grad_weight * grad_loss\n",
    "        return total_loss\n",
    "\n",
    "    def ssim(self, predicted, target):\n",
    "        \"\"\"\n",
    "        Calculate the Structural Similarity Index (SSIM) between predicted and target for the whole batch.\n",
    "        \"\"\"\n",
    "        predicted_np = predicted.cpu().detach().numpy()\n",
    "        target_np = target.cpu().detach().numpy()\n",
    "\n",
    "        batch_size = predicted.shape[0]\n",
    "        ssim_loss = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            predicted_image = np.clip(predicted_np[i, 0, :, :], 0, 1)\n",
    "            target_image = np.clip(target_np[i, 0, :, :], 0, 1)\n",
    "            try:\n",
    "                ssim_value = ssim(predicted_image, target_image, data_range=1.0, win_size=7)\n",
    "                ssim_loss += (1 - ssim_value)\n",
    "            except ValueError:\n",
    "                ssim_loss += 1\n",
    "        \n",
    "        ssim_loss /= batch_size\n",
    "        return ssim_loss\n",
    "\n",
    "    def l1(self, predicted, target):\n",
    "        \"\"\"Calculate the L1 Loss between predicted and target.\"\"\"\n",
    "        return F.l1_loss(predicted, target)\n",
    "        \n",
    "    def gradient_loss(self, predicted, target):\n",
    "        \"\"\"Calculate the gradient loss.\"\"\"\n",
    "        grad_pred_x = predicted[:, :, 1:, :] - predicted[:, :, :-1, :]\n",
    "        grad_pred_y = predicted[:, :, :, 1:] - predicted[:, :, :, :-1]\n",
    "\n",
    "        grad_target_x = target[:, :, 1:, :] - target[:, :, :-1, :]\n",
    "        grad_target_y = target[:, :, :, 1:] - target[:, :, :, :-1]\n",
    "\n",
    "        grad_loss_x = F.l1_loss(grad_pred_x, grad_target_x)\n",
    "        grad_loss_y = F.l1_loss(grad_pred_y, grad_target_y)\n",
    "\n",
    "        return grad_loss_x + grad_loss_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:37.336090Z",
     "iopub.status.busy": "2024-12-20T03:31:37.335162Z",
     "iopub.status.idle": "2024-12-20T03:31:37.352704Z",
     "shell.execute_reply": "2024-12-20T03:31:37.351842Z",
     "shell.execute_reply.started": "2024-12-20T03:31:37.336054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the CombinedLoss\n",
    "criterion = CombinedLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging into Weights & Biases (W&B) Using a User Secret API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:37.354234Z",
     "iopub.status.busy": "2024-12-20T03:31:37.353821Z",
     "iopub.status.idle": "2024-12-20T03:31:40.569693Z",
     "shell.execute_reply": "2024-12-20T03:31:40.568825Z",
     "shell.execute_reply.started": "2024-12-20T03:31:37.354199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a generic user secret\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    " \n",
    "#Login to W&B using the retrieved API key\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Uploading a PyTorch Model to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T03:31:40.571179Z",
     "iopub.status.busy": "2024-12-20T03:31:40.570698Z",
     "iopub.status.idle": "2024-12-20T03:31:41.365776Z",
     "shell.execute_reply": "2024-12-20T03:31:41.364975Z",
     "shell.execute_reply.started": "2024-12-20T03:31:40.571152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)\n",
    "\n",
    "repo_name = \"ConvNeXt_ambient_occlusion_model_1\"\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "\n",
    "def save_to_huggingface(model):\n",
    "    # Save the model to a .pth file\n",
    "    save_path = \"ConvNeXt_ambient_occlusion_model_1.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved locally to {save_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj=save_path,\n",
    "        path_in_repo=save_path,\n",
    "        repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "        token=hf_token  # Using the token from environment variable\n",
    "    )\n",
    "    print(f\"Model uploaded to Hugging Face Hub: https://huggingface.co/prakanda/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-20T03:38:02.109Z",
     "iopub.execute_input": "2024-12-20T03:37:41.023419Z",
     "iopub.status.busy": "2024-12-20T03:37:41.022586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device0=\"cuda:0\"\n",
    "num_epochs = 10\n",
    "wandb.init(project=\"ambient-occlusion-ConvNeXt\", config={\"epochs\": 10, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "\n",
    "# Move the model to the GPU before the training loop\n",
    "model.to(device0) \n",
    "\n",
    "max_grad_norm = 1.0  # Set the gradient clipping norm\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Move inputs, targets, and camera_intrinsic to the device\n",
    "        inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "        targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        predicted_ambient = outputs[-1]\n",
    "\n",
    "        # Use the criterion for ambient loss calculation\n",
    "        loss = criterion(predicted_ambient, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log progress every 10 batches\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            wandb.log({\"epoch\": epoch + 1, \"batch_loss\": loss.item()})\n",
    "\n",
    "        # Save the model every 1000 batches\n",
    "        if batch_idx != 0 and batch_idx % 1000 == 0:\n",
    "            print(\"Saving model\")\n",
    "            save_to_huggingface(model)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    average_loss_per_epoch = running_loss / len(train_dataloader)\n",
    "    wandb.log({\"epoch\": epoch + 1, \"average_loss_per_epoch\": average_loss_per_epoch})\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss_per_epoch:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T03:31:46.765646Z",
     "iopub.status.idle": "2024-12-20T03:31:46.765955Z",
     "shell.execute_reply": "2024-12-20T03:31:46.765827Z",
     "shell.execute_reply.started": "2024-12-20T03:31:46.765796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Log in using the token\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T03:31:46.766968Z",
     "iopub.status.idle": "2024-12-20T03:31:46.767363Z",
     "shell.execute_reply": "2024-12-20T03:31:46.767225Z",
     "shell.execute_reply.started": "2024-12-20T03:31:46.767208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download the model file from Hugging Face Hub\n",
    "repo_name = \"ConvNeXt_ambient_occlusion_model_1\"\n",
    "downloaded_file = hf_hub_download(\n",
    "    repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n",
    "    filename=\"ConvNeXt_ambient_occlusion_model_1.pth\"\n",
    ")\n",
    "print(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and load the state_dict\n",
    "\n",
    "model.load_state_dict(torch.load(downloaded_file),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T03:31:46.768382Z",
     "iopub.status.idle": "2024-12-20T03:31:46.768724Z",
     "shell.execute_reply": "2024-12-20T03:31:46.768594Z",
     "shell.execute_reply.started": "2024-12-20T03:31:46.768578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device0=\"cuda:0\"\n",
    "wandb.init(project=\"ambient-occlusion-ConvNeXt\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n",
    "\n",
    "def evaluate(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # Accumulate loss over all batches\n",
    "    total_samples = 0   # Track the number of processed samples\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_dataloader):\n",
    "            # Move inputs and targets to the device\n",
    "            inputs = batch[\"input\"].to(device0, non_blocking=True)\n",
    "            targets = batch[\"target\"].to(device0, non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            predicted_ambient= outputs[-1]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted_ambient, targets)\n",
    "\n",
    "            # Accumulate running loss and sample count\n",
    "            running_loss += loss.item() * inputs.size(0)  # Weighted by batch size\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Log progress every 10 batches\n",
    "            if batch_idx % 40 == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                wandb.log({\"batch_number\": batch_idx,\"batch_loss\": loss.item()})\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate average loss over the dataset\n",
    "        avg_loss = running_loss / total_samples\n",
    "        return avg_loss\n",
    "\n",
    "# Perform evaluation on the test set\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Log the average test loss to W&B\n",
    "wandb.log({\"average_test_loss\": test_loss})\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6211511,
     "sourceId": 10076639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220263,
     "sourceId": 10088214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6220624,
     "sourceId": 10088660,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6222017,
     "sourceId": 10090456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256336,
     "sourceId": 10137083,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6279702,
     "sourceId": 10186733,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6321457,
     "sourceId": 10225130,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
