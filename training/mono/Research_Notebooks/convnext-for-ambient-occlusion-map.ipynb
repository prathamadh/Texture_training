{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10076639,"sourceType":"datasetVersion","datasetId":6211511},{"sourceId":10088214,"sourceType":"datasetVersion","datasetId":6220263},{"sourceId":10088660,"sourceType":"datasetVersion","datasetId":6220624},{"sourceId":10090456,"sourceType":"datasetVersion","datasetId":6222017},{"sourceId":10137083,"sourceType":"datasetVersion","datasetId":6256336},{"sourceId":10186733,"sourceType":"datasetVersion","datasetId":6279702},{"sourceId":10225130,"sourceType":"datasetVersion","datasetId":6321457}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Necessary Installations","metadata":{}},{"cell_type":"code","source":"! pip install wandb\n! pip install scikit-image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:29:49.916389Z","iopub.execute_input":"2024-12-20T03:29:49.916985Z","iopub.status.idle":"2024-12-20T03:30:07.469424Z","shell.execute_reply.started":"2024-12-20T03:29:49.916955Z","shell.execute_reply":"2024-12-20T03:30:07.468391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_file\nfrom huggingface_hub import login\nimport torch.nn.functional as F\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.registry import register_model\nfrom huggingface_hub import hf_hub_download\nfrom skimage.metrics import structural_similarity as ssim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:07.471522Z","iopub.execute_input":"2024-12-20T03:30:07.471841Z","iopub.status.idle":"2024-12-20T03:30:13.957505Z","shell.execute_reply.started":"2024-12-20T03:30:07.471788Z","shell.execute_reply":"2024-12-20T03:30:13.956661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Model Setup and Loading Checkpoints","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass AOHead(nn.Module):\n    def __init__(self, in_channels=768, out_channels=1):\n        super(AOHead, self).__init__()\n        \n        # Convolutional layers for further refinement of features\n        self.conv1 = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n        \n        # Output layer: Single channel for the AO map\n        self.conv_out = nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=1)\n\n        # Activation functions\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()  # For output normalization to range [0, 1]\n        \n        # Upsample to 600x600\n        self.upsample = nn.Upsample(size=(600, 600), mode='bilinear', align_corners=True)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the AO head.\n        \n        Parameters:\n            x (torch.Tensor): The input feature map from the ConvNeXt backbone.\n        \n        Returns:\n            torch.Tensor: The predicted Ambient Occlusion map.\n        \"\"\"\n        # Pass through the convolutional layers with ReLU activations\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.relu(self.conv4(x))\n        \n        # Final output (predicting AO map)\n        x = self.conv_out(x)\n        \n        # Apply sigmoid to get values in the range [0, 1] (for visualizing AO)\n        x = self.sigmoid(x)\n        \n        # Upsample to (600, 600)\n        x = self.upsample(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:13.958766Z","iopub.execute_input":"2024-12-20T03:30:13.959329Z","iopub.status.idle":"2024-12-20T03:30:13.966617Z","shell.execute_reply.started":"2024-12-20T03:30:13.959300Z","shell.execute_reply":"2024-12-20T03:30:13.965851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n\n    Args:\n        dim (int): Number of input channels.\n        drop_path (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n                                    requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:13.967597Z","iopub.execute_input":"2024-12-20T03:30:13.967866Z","iopub.status.idle":"2024-12-20T03:30:13.985281Z","shell.execute_reply.started":"2024-12-20T03:30:13.967835Z","shell.execute_reply":"2024-12-20T03:30:13.984626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n    def __init__(self, in_chans=3, out_chans=1,\n                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n                 layer_scale_init_value=1e-6, head_init_scale=1.,\n                 **kwargs,):\n\n        super().__init__()\n\n\n\n        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n\n\n        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n\n\n\n        # Output head for ambient map prediction\n        self.ao_head = AOHead(in_channels= dims[-1], out_channels=1)\n\n\n        self.apply(self._init_weights)\n\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        features = []\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n            features.append(x)\n\n        x = self.ao_head(x)\n        features.append(x)\n\n        return features\n        \n\n    def forward(self, x):\n        features = self.forward_features(x)\n\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:13.987030Z","iopub.execute_input":"2024-12-20T03:30:13.987264Z","iopub.status.idle":"2024-12-20T03:30:14.001975Z","shell.execute_reply.started":"2024-12-20T03:30:13.987241Z","shell.execute_reply":"2024-12-20T03:30:14.001089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_urls = {\n    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:14.002895Z","iopub.execute_input":"2024-12-20T03:30:14.003209Z","iopub.status.idle":"2024-12-20T03:30:14.016357Z","shell.execute_reply.started":"2024-12-20T03:30:14.003173Z","shell.execute_reply":"2024-12-20T03:30:14.015648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n    with shape (batch_size, channels, height, width).\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError\n        self.normalized_shape = (normalized_shape, )\n\n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n\n            return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:14.017353Z","iopub.execute_input":"2024-12-20T03:30:14.017614Z","iopub.status.idle":"2024-12-20T03:30:14.028209Z","shell.execute_reply.started":"2024-12-20T03:30:14.017589Z","shell.execute_reply":"2024-12-20T03:30:14.027587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convnext_tiny(pretrained=True,in_22k=False, **kwargs):\n    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n    if pretrained:\n        #checkpoint = torch.load(kwargs['checkpoint'], map_location=\"cpu\")\n        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n        model_dict = model.state_dict()\n        pretrained_dict = {}\n        unmatched_pretrained_dict = {}\n        for k, v in checkpoint['model'].items():\n            if k in model_dict:\n                pretrained_dict[k] = v\n            else:\n                unmatched_pretrained_dict[k] = v\n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n        \n\n        print(f'The type of checkpoint is {type(checkpoint)}.')\n        print(f'The type of model.state_dict is {type(model.state_dict)}.')\n        print(f'The type of model.state_dict() is {type(model.state_dict())}.')\n\n\n        \n        for name,param in model.named_parameters():\n          if name in pretrained_dict.keys():\n              param.requires_grad = True\n          else :\n              param.requires_grad = True\n\n\n        print('\\n')\n\n        print(f'The keys in pretrained_dict are : \\n {pretrained_dict.keys()}')\n\n        print('\\n')\n\n        print(f'The keys in unmatched_pretrained_dict are : \\n {unmatched_pretrained_dict.keys()}')\n\n        print('\\n')\n\n        print(\n            'Successfully loaded pretrained %d paras, and %d paras are unmatched.'\n            %(len(pretrained_dict.keys()), len(unmatched_pretrained_dict.keys())))\n\n        print('\\n')\n\n        print('Unmatched pretrained paras are:', unmatched_pretrained_dict.keys())\n        \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:14.029168Z","iopub.execute_input":"2024-12-20T03:30:14.029462Z","iopub.status.idle":"2024-12-20T03:30:14.042909Z","shell.execute_reply.started":"2024-12-20T03:30:14.029413Z","shell.execute_reply":"2024-12-20T03:30:14.042147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Dataset Definition","metadata":{}},{"cell_type":"code","source":"# Define a custom dataset class for loading texture and ambient data\nclass TextureDataset(Dataset):\n    def __init__(self, texture_paths, ambient_paths, transform=None):\n        \"\"\"\n        Args:\n            texture_paths (list): List of paths to texture images.\n            ambient_paths (list): List of paths to ambient images.\n            transform (callable, optional): Optional transform to be applied\n                on both input and target images.\n        \"\"\"\n        self.texture_paths = texture_paths\n        self.ambient_paths = ambient_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.texture_paths)\n\n    def __getitem__(self, idx):\n        # Load images\n        texture = Image.open(self.texture_paths[idx]).convert(\"RGB\")\n        ambient = Image.open(self.ambient_paths[idx]).convert(\"L\")\n\n        # Apply transforms if defined\n        if self.transform:\n            texture = self.transform(texture)\n            ambient = self.transform(ambient)\n\n\n        return {\"input\": texture, \"target\": ambient}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:14.043782Z","iopub.execute_input":"2024-12-20T03:30:14.044024Z","iopub.status.idle":"2024-12-20T03:30:14.057114Z","shell.execute_reply.started":"2024-12-20T03:30:14.044002Z","shell.execute_reply":"2024-12-20T03:30:14.056459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Extract Texture and Ambient Paths from CSV Files","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n# Specify the directory containing your CSV files\ncsv_directory = \"/kaggle/input/texture-allpathds\"  # Replace with your directory path\n\n# Initialize lists to store paths\ntexture_paths = []\nambient_paths = []\n\n# Iterate over all CSV files in the directory\nfor file_name in os.listdir(csv_directory):\n    if file_name.endswith(\".csv\"):  # Check if the file is a CSV\n        file_path = os.path.join(csv_directory, file_name)\n        \n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        \n        df[\"Ambient\"] = df[\"Depth\"].apply(lambda x: x.replace(\"/sa_\", \"/AOsa_\"))\n\n        \n        # Function to replace the path\n        def replace_path(path):\n            # Extract the file name\n            filename = os.path.basename(path)\n            \n            # Construct the new path\n            new_path = f\"/kaggle/input/ambinet-occlusion/{filename}\"\n            return new_path\n        \n        # Apply the function to the column\n        df[\"Ambient\"] = df[\"Ambient\"].apply(replace_path)\n\n        # Check if the paths exist\n        df = df[df[\"Ambient\"].apply(os.path.exists)]\n\n\n        # Extract columns and append to the lists\n        if \"Texture\" in df.columns and \"Ambient\" in df.columns:\n            texture_paths.extend(df[\"Texture\"].dropna().tolist())\n\n            \n            ambient_paths.extend(df[\"Ambient\"].dropna().tolist())\n        else:\n            print(f\"Warning: File {file_name} does not contain 'texture' column.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:30:14.058087Z","iopub.execute_input":"2024-12-20T03:30:14.058308Z","iopub.status.idle":"2024-12-20T03:31:34.503523Z","shell.execute_reply.started":"2024-12-20T03:30:14.058287Z","shell.execute_reply":"2024-12-20T03:31:34.502775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"# Split data into training and testing sets\ntrain_textures, test_textures, train_ambient, test_ambient = train_test_split(\n    texture_paths, ambient_paths, test_size=0.01, random_state=2\n)\n\n# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((600, 600)),  # Resize images\n    transforms.ToTensor(),          # Convert images to tensors\n])\n\n# Create training and testing datasets\ntrain_dataset = TextureDataset(train_textures, train_ambient, transform=transform)\ntest_dataset = TextureDataset(test_textures, test_ambient, transform=transform)\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:34.504436Z","iopub.execute_input":"2024-12-20T03:31:34.504666Z","iopub.status.idle":"2024-12-20T03:31:34.517792Z","shell.execute_reply.started":"2024-12-20T03:31:34.504642Z","shell.execute_reply":"2024-12-20T03:31:34.517171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Model Instantiation","metadata":{}},{"cell_type":"code","source":"model = convnext_tiny(True, in_22k=False).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:34.518637Z","iopub.execute_input":"2024-12-20T03:31:34.518958Z","iopub.status.idle":"2024-12-20T03:31:37.303671Z","shell.execute_reply.started":"2024-12-20T03:31:34.518917Z","shell.execute_reply":"2024-12-20T03:31:37.302333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Training Configuration","metadata":{}},{"cell_type":"code","source":"# Set optimizer\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:37.304732Z","iopub.execute_input":"2024-12-20T03:31:37.305063Z","iopub.status.idle":"2024-12-20T03:31:37.312350Z","shell.execute_reply.started":"2024-12-20T03:31:37.305035Z","shell.execute_reply":"2024-12-20T03:31:37.311186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Combined Loss Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, ssim_weight=0.5, l1_weight=0.25, grad_weight=0.25):\n        super(CombinedLoss, self).__init__()\n        self.ssim_weight = ssim_weight\n        self.l1_weight = l1_weight\n        self.grad_weight = grad_weight\n\n    def forward(self, predicted, target):\n        # Ensure shapes match\n        if predicted.shape != target.shape:\n            raise ValueError(f\"Shape mismatch: predicted {predicted.shape}, target {target.shape}\")\n        \n        # Calculate individual losses\n        ssim_loss = self.ssim(predicted, target)\n        l1_loss = self.l1(predicted, target)\n        grad_loss = self.gradient_loss(predicted, target)\n        \n        # Combine the losses with the specified weights\n        total_loss = self.ssim_weight * ssim_loss + self.l1_weight * l1_loss + self.grad_weight * grad_loss\n        return total_loss\n\n    def ssim(self, predicted, target):\n        \"\"\"\n        Calculate the Structural Similarity Index (SSIM) between predicted and target for the whole batch.\n        \"\"\"\n        predicted_np = predicted.cpu().detach().numpy()\n        target_np = target.cpu().detach().numpy()\n\n        batch_size = predicted.shape[0]\n        ssim_loss = 0\n        \n        for i in range(batch_size):\n            predicted_image = np.clip(predicted_np[i, 0, :, :], 0, 1)\n            target_image = np.clip(target_np[i, 0, :, :], 0, 1)\n            try:\n                ssim_value = ssim(predicted_image, target_image, data_range=1.0, win_size=7)\n                ssim_loss += (1 - ssim_value)\n            except ValueError:\n                ssim_loss += 1\n        \n        ssim_loss /= batch_size\n        return ssim_loss\n\n    def l1(self, predicted, target):\n        \"\"\"Calculate the L1 Loss between predicted and target.\"\"\"\n        return F.l1_loss(predicted, target)\n        \n    def gradient_loss(self, predicted, target):\n        \"\"\"Calculate the gradient loss.\"\"\"\n        grad_pred_x = predicted[:, :, 1:, :] - predicted[:, :, :-1, :]\n        grad_pred_y = predicted[:, :, :, 1:] - predicted[:, :, :, :-1]\n\n        grad_target_x = target[:, :, 1:, :] - target[:, :, :-1, :]\n        grad_target_y = target[:, :, :, 1:] - target[:, :, :, :-1]\n\n        grad_loss_x = F.l1_loss(grad_pred_x, grad_target_x)\n        grad_loss_y = F.l1_loss(grad_pred_y, grad_target_y)\n\n        return grad_loss_x + grad_loss_y\n\n\n# Initialize Model with Proper Weight Initialization\ndef initialize_weights(model):\n    \"\"\"\n    Applies Xavier initialization for weights and zeroes for biases to ensure \n    proper gradient flow in the network.\n    \"\"\"\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:37.317289Z","iopub.execute_input":"2024-12-20T03:31:37.318113Z","iopub.status.idle":"2024-12-20T03:31:37.334039Z","shell.execute_reply.started":"2024-12-20T03:31:37.318069Z","shell.execute_reply":"2024-12-20T03:31:37.333034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the CombinedLoss\ncriterion = CombinedLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:37.335162Z","iopub.execute_input":"2024-12-20T03:31:37.336090Z","iopub.status.idle":"2024-12-20T03:31:37.352704Z","shell.execute_reply.started":"2024-12-20T03:31:37.336054Z","shell.execute_reply":"2024-12-20T03:31:37.351842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Logging into Weights & Biases (W&B) Using a User Secret API Key","metadata":{}},{"cell_type":"code","source":"# Define a generic user secret\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n \n#Login to W&B using the retrieved API key\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:37.353821Z","iopub.execute_input":"2024-12-20T03:31:37.354234Z","iopub.status.idle":"2024-12-20T03:31:40.569693Z","shell.execute_reply.started":"2024-12-20T03:31:37.354199Z","shell.execute_reply":"2024-12-20T03:31:40.568825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Saving and Uploading a PyTorch Model to the Hugging Face Hub","metadata":{}},{"cell_type":"code","source":"# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n\n# Log in using the token\nlogin(token=hf_token)\n\nrepo_name = \"ConvNeXt_ambient_occlusion_model_1\"\ncreate_repo(repo_name, exist_ok=True)\n\ndef save_to_huggingface(model):\n    # Save the model to a .pth file\n    save_path = \"ConvNeXt_ambient_occlusion_model_1.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Model saved locally to {save_path}\")\n\n    upload_file(\n        path_or_fileobj=save_path,\n        path_in_repo=save_path,\n        repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n        token=hf_token  # Using the token from environment variable\n    )\n    print(f\"Model uploaded to Hugging Face Hub: https://huggingface.co/prakanda/{repo_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:40.570698Z","iopub.execute_input":"2024-12-20T03:31:40.571179Z","iopub.status.idle":"2024-12-20T03:31:41.365776Z","shell.execute_reply.started":"2024-12-20T03:31:40.571152Z","shell.execute_reply":"2024-12-20T03:31:41.364975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"device0=\"cuda:0\"\nnum_epochs = 10\nwandb.init(project=\"ambient-occlusion-ConvNeXt\", config={\"epochs\": 10, \"batch_size\": 2, \"learning_rate\": 1e-4})\n\n\n# Move the model to the GPU before the training loop\nmodel.to(device0) \n\nmax_grad_norm = 1.0  # Set the gradient clipping norm\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move inputs, targets, and camera_intrinsic to the device\n        inputs = batch[\"input\"].to(device0, non_blocking=True)\n        targets = batch[\"target\"].to(device0, non_blocking=True)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n\n        predicted_ambient = outputs[-1]\n\n        # Use the criterion for ambient loss calculation\n        loss = criterion(predicted_ambient, targets)\n\n        # Backward pass and optimization\n        loss.backward()\n\n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        \n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Log progress every 10 batches\n        if batch_idx % 10 == 0:\n            print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n\n        wandb.log({\"epoch\": epoch + 1, \"batch_loss\": loss.item()})\n\n        # Save the model every 1000 batches\n        if batch_idx != 0 and batch_idx % 1000 == 0:\n            print(\"Saving model\")\n            save_to_huggingface(model)\n\n    # Log metrics to W&B\n    average_loss_per_epoch = running_loss / len(train_dataloader)\n    wandb.log({\"epoch\": epoch + 1, \"average_loss_per_epoch\": average_loss_per_epoch})\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss_per_epoch:.4f}\")\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:37:41.022586Z","iopub.execute_input":"2024-12-20T03:37:41.023419Z","execution_failed":"2024-12-20T03:38:02.109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Loading Model from Hugging Face","metadata":{}},{"cell_type":"code","source":"# Set the Hugging Face token from environment variables (ensure it's set in your Kaggle environment)\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n\n# Log in using the token\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:46.765646Z","iopub.status.idle":"2024-12-20T03:31:46.765955Z","shell.execute_reply.started":"2024-12-20T03:31:46.765796Z","shell.execute_reply":"2024-12-20T03:31:46.765827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download the model file from Hugging Face Hub\nrepo_name = \"ConvNeXt_ambient_occlusion_model_1\"\ndownloaded_file = hf_hub_download(\n    repo_id=f\"prakanda/{repo_name}\",  # Replace with your Hugging Face username\n    filename=\"ConvNeXt_ambient_occlusion_model_1.pth\"\n)\nprint(f\"Model downloaded from Hugging Face Hub: {downloaded_file}\")\n\n\n\n# Initialize the model and load the state_dict\n\nmodel.load_state_dict(torch.load(downloaded_file),strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:46.766968Z","iopub.status.idle":"2024-12-20T03:31:46.767363Z","shell.execute_reply.started":"2024-12-20T03:31:46.767208Z","shell.execute_reply":"2024-12-20T03:31:46.767225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Evaluation Loop","metadata":{}},{"cell_type":"code","source":"device0=\"cuda:0\"\nwandb.init(project=\"ambient-occlusion-ConvNeXt\", config={\"epochs\": 1, \"batch_size\": 2, \"learning_rate\": 1e-4})\n\ndef evaluate(model, test_dataloader, criterion):\n    model.eval()\n    running_loss = 0.0  # Accumulate loss over all batches\n    total_samples = 0   # Track the number of processed samples\n\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_dataloader):\n            # Move inputs and targets to the device\n            inputs = batch[\"input\"].to(device0, non_blocking=True)\n            targets = batch[\"target\"].to(device0, non_blocking=True)\n\n            # Forward pass\n            outputs = model(inputs)\n            predicted_ambient= outputs[-1]\n\n            # Calculate loss\n            loss = criterion(predicted_ambient, targets)\n\n            # Accumulate running loss and sample count\n            running_loss += loss.item() * inputs.size(0)  # Weighted by batch size\n            total_samples += inputs.size(0)\n\n        # Calculate average loss over the dataset\n        avg_loss = running_loss / total_samples\n        return avg_loss\n\n# Perform evaluation on the test set\ntest_loss = evaluate(model, test_dataloader, criterion)\nprint(f\"Test Loss: {test_loss:.4f}\")\n\n# Log the average test loss to W&B\nwandb.log({\"average_test_loss\": test_loss})\n\n# Finish W&B run\nwandb.finish()\n\nprint(\"Evaluation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T03:31:46.768382Z","iopub.status.idle":"2024-12-20T03:31:46.768724Z","shell.execute_reply.started":"2024-12-20T03:31:46.768578Z","shell.execute_reply":"2024-12-20T03:31:46.768594Z"}},"outputs":[],"execution_count":null}]}